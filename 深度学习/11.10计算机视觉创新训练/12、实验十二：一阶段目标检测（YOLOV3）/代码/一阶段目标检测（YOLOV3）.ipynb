{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 任务描述：\n",
    "<font size=2.5>\n",
    "  \n",
    "本示例教程介绍如何使用飞桨完成一个目标检测任务。\n",
    "   \n",
    "## 读取AI识虫数据集标注信息\n",
    "\n",
    "AI识虫数据集结构如下：\n",
    "\n",
    "* 提供了2183张图片，其中训练集1693张，验证集245，测试集245张。\n",
    "* 包含7种昆虫，分别是Boerner、Leconte、Linnaeus、acuminatus、armandi、coleoptera和linnaeus。\n",
    "* 包含了图片和标注，请读者先将数据解压，并存放在insects目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据脚本，将文件解压到work目录下\n",
    "# !unzip -o -q -d  /home/aistudio/work /home/aistudio/data/data19638/insects.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将数据解压之后，可以看到insects目录下的结构如下所示。\n",
    "\n",
    "        insects\n",
    "            |---train\n",
    "            |         |---annotations\n",
    "            |         |         |---xmls\n",
    "            |         |                  |---100.xml\n",
    "            |         |                  |---101.xml\n",
    "            |         |                  |---...\n",
    "            |         |\n",
    "            |         |---images\n",
    "            |                   |---100.jpeg\n",
    "            |                   |---101.jpeg\n",
    "            |                   |---...\n",
    "            |\n",
    "            |---val\n",
    "            |        |---annotations\n",
    "            |        |         |---xmls\n",
    "            |        |                  |---1221.xml\n",
    "            |        |                  |---1277.xml\n",
    "            |        |                  |---...\n",
    "            |        |\n",
    "            |        |---images\n",
    "            |                  |---1221.jpeg\n",
    "            |                  |---1277.jpeg\n",
    "            |                  |---...\n",
    "            |\n",
    "            |---test\n",
    "                     |---images\n",
    "                               |---1833.jpeg\n",
    "                               |---1838.jpeg\n",
    "                               |---...\n",
    "\n",
    "<font size=3>\n",
    "  \n",
    "insects包含train、val和test三个文件夹。train/annotations/xmls目录下存放着图片的标注。  \n",
    "  每个xml文件是对一张图片的说明，包括图片尺寸、包含的昆虫名称、在图片上出现的位置等信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "<annotation>\n",
    "        <folder>刘霏霏</folder>\n",
    "        <filename>100.jpeg</filename>\n",
    "        <path>/home/fion/桌面/刘霏霏/100.jpeg</path>\n",
    "        <source>\n",
    "                <database>Unknown</database>\n",
    "        </source>\n",
    "        <size>\n",
    "                <width>1336</width>\n",
    "                <height>1336</height>\n",
    "                <depth>3</depth>\n",
    "        </size>\n",
    "        <segmented>0</segmented>\n",
    "        <object>\n",
    "                <name>Boerner</name>\n",
    "                <pose>Unspecified</pose>\n",
    "                <truncated>0</truncated>\n",
    "                <difficult>0</difficult>\n",
    "                <bndbox>\n",
    "                        <xmin>500</xmin>\n",
    "                        <ymin>893</ymin>\n",
    "                        <xmax>656</xmax>\n",
    "                        <ymax>966</ymax>\n",
    "                </bndbox>\n",
    "        </object>\n",
    "        <object>\n",
    "                <name>Leconte</name>\n",
    "                <pose>Unspecified</pose>\n",
    "                <truncated>0</truncated>\n",
    "                <difficult>0</difficult>\n",
    "                <bndbox>\n",
    "                        <xmin>622</xmin>\n",
    "                        <ymin>490</ymin>\n",
    "                        <xmax>756</xmax>\n",
    "                        <ymax>610</ymax>\n",
    "                </bndbox>\n",
    "        </object>\n",
    "        <object>\n",
    "                <name>armandi</name>\n",
    "                <pose>Unspecified</pose>\n",
    "                <truncated>0</truncated>\n",
    "                <difficult>0</difficult>\n",
    "                <bndbox>\n",
    "                        <xmin>432</xmin>\n",
    "                        <ymin>663</ymin>\n",
    "                        <xmax>517</xmax>\n",
    "                        <ymax>729</ymax>\n",
    "                </bndbox>\n",
    "        </object>\n",
    "        <object>\n",
    "                <name>coleoptera</name>\n",
    "                <pose>Unspecified</pose>\n",
    "                <truncated>0</truncated>\n",
    "                <difficult>0</difficult>\n",
    "                <bndbox>\n",
    "                        <xmin>624</xmin>\n",
    "                        <ymin>685</ymin>\n",
    "                        <xmax>697</xmax>\n",
    "                        <ymax>771</ymax>\n",
    "                </bndbox>\n",
    "        </object>\n",
    "        <object>\n",
    "                <name>linnaeus</name>\n",
    "                <pose>Unspecified</pose>\n",
    "                <truncated>0</truncated>\n",
    "                <difficult>0</difficult>\n",
    "                <bndbox>\n",
    "                        <xmin>783</xmin>\n",
    "                        <ymin>700</ymin>\n",
    "                        <xmax>856</xmax>\n",
    "                        <ymax>802</ymax>\n",
    "                </bndbox>\n",
    "        </object>\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "上面列出的xml文件中的主要参数说明如下：\n",
    "\n",
    "* size：图片尺寸。\n",
    "\n",
    "* object：图片中包含的物体，一张图片可能中包含多个物体。\n",
    "\n",
    " -- name：昆虫名称；\n",
    " \n",
    " -- bndbox：物体真实框；\n",
    " \n",
    " -- difficult：识别是否困难。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "  下面我们将从数据集中读取xml文件，将每张图片的标注信息读取出来。  \n",
    "  在读取具体的标注文件之前，我们先完成一件事情，就是将昆虫的类别名字（字符串）转化成数字表示的类别。  \n",
    "  因为神经网络里面计算时需要的输入类型是数值型的，所以需要将字符串表示的类别转化成具体的数字。  \n",
    "  昆虫类别名称的列表是：['Boerner', 'Leconte', 'Linnaeus', 'acuminatus', 'armandi', 'coleoptera', 'linnaeus']，这里我们约定此列表中：'Boerner'对应类别0，'Leconte'对应类别1，...，'linnaeus'对应类别6。使用下面的程序可以得到表示名称字符串和数字类别之间映射关系的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INSECT_NAMES = ['Boerner', 'Leconte', 'Linnaeus', \n",
    "                'acuminatus', 'armandi', 'coleoptera', 'linnaeus']\n",
    "\n",
    "def get_insect_names():\n",
    "    \"\"\"\n",
    "    return a dict, as following,\n",
    "        {'Boerner': 0,\n",
    "         'Leconte': 1,\n",
    "         'Linnaeus': 2, \n",
    "         'acuminatus': 3,\n",
    "         'armandi': 4,\n",
    "         'coleoptera': 5,\n",
    "         'linnaeus': 6\n",
    "        }\n",
    "    It can map the insect name into an integer label.\n",
    "    \"\"\"\n",
    "    insect_category2id = {}\n",
    "    for i, item in enumerate(INSECT_NAMES):\n",
    "        insect_category2id[item] = i\n",
    "\n",
    "    return insect_category2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Boerner': 0,\n",
       " 'Leconte': 1,\n",
       " 'Linnaeus': 2,\n",
       " 'acuminatus': 3,\n",
       " 'armandi': 4,\n",
       " 'coleoptera': 5,\n",
       " 'linnaeus': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cname2cid = get_insect_names()\n",
    "cname2cid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "调用get_insect_names函数返回一个dict，描述了昆虫名称和数字类别之间的映射关系。下面的程序从`annotations/xml`目录下面读取所有文件标注信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_annotations(cname2cid, datadir):\n",
    "    filenames = os.listdir(os.path.join(datadir, 'annotations', 'xmls'))\n",
    "    records = []\n",
    "    ct = 0\n",
    "    for fname in filenames:\n",
    "        fid = fname.split('.')[0]\n",
    "        fpath = os.path.join(datadir, 'annotations', 'xmls', fname)\n",
    "        img_file = os.path.join(datadir, 'images', fid + '.jpeg')\n",
    "        tree = ET.parse(fpath)\n",
    "\n",
    "        if tree.find('id') is None:\n",
    "            im_id = np.array([ct])\n",
    "        else:\n",
    "            im_id = np.array([int(tree.find('id').text)])\n",
    "\n",
    "        objs = tree.findall('object')\n",
    "        im_w = float(tree.find('size').find('width').text)\n",
    "        im_h = float(tree.find('size').find('height').text)\n",
    "        gt_bbox = np.zeros((len(objs), 4), dtype=np.float32)\n",
    "        gt_class = np.zeros((len(objs), ), dtype=np.int32)\n",
    "        is_crowd = np.zeros((len(objs), ), dtype=np.int32)\n",
    "        difficult = np.zeros((len(objs), ), dtype=np.int32)\n",
    "        for i, obj in enumerate(objs):\n",
    "            cname = obj.find('name').text\n",
    "            gt_class[i] = cname2cid[cname]\n",
    "            _difficult = int(obj.find('difficult').text)\n",
    "            x1 = float(obj.find('bndbox').find('xmin').text)\n",
    "            y1 = float(obj.find('bndbox').find('ymin').text)\n",
    "            x2 = float(obj.find('bndbox').find('xmax').text)\n",
    "            y2 = float(obj.find('bndbox').find('ymax').text)\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(im_w - 1, x2)\n",
    "            y2 = min(im_h - 1, y2)\n",
    "            # 这里使用xywh格式来表示目标物体真实框\n",
    "            gt_bbox[i] = [(x1+x2)/2.0 , (y1+y2)/2.0, x2-x1+1., y2-y1+1.]\n",
    "            is_crowd[i] = 0\n",
    "            difficult[i] = _difficult\n",
    "\n",
    "        voc_rec = {\n",
    "            'im_file': img_file,\n",
    "            'im_id': im_id,\n",
    "            'h': im_h,\n",
    "            'w': im_w,\n",
    "            'is_crowd': is_crowd,\n",
    "            'gt_class': gt_class,\n",
    "            'gt_bbox': gt_bbox,\n",
    "            'gt_poly': [],\n",
    "            'difficult': difficult\n",
    "            }\n",
    "        if len(objs) != 0:\n",
    "            records.append(voc_rec)\n",
    "        ct += 1\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "通过上面的程序，将所有训练数据集的标注数据全部读取出来了，存放在records列表下面，其中每一个元素是一张图片的标注数据，包含了图片存放地址，图片id，图片高度和宽度，图片中所包含的目标物体的种类和位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据读取和预处理\n",
    "<font size=3>\n",
    "\n",
    "数据预处理是训练神经网络时非常重要的步骤。合适的预处理方法，可以帮助模型更好的收敛并防止过拟合。首先我们需要从磁盘读入数据，然后需要对这些数据进行预处理，为了保证网络运行的速度，通常还要对数据预处理进行加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读取\n",
    "<font size=3>\n",
    "\n",
    "前面已经将图片的所有描述信息保存在records中了，其中每一个元素都包含了一张图片的描述，下面的程序展示了如何根据records里面的描述读取图片及标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "import cv2\n",
    "\n",
    "def get_bbox(gt_bbox, gt_class):\n",
    "    # 对于一般的检测任务来说，一张图片上往往会有多个目标物体\n",
    "    # 设置参数MAX_NUM = 50， 即一张图片最多取50个真实框；如果真实\n",
    "    # 框的数目少于50个，则将不足部分的gt_bbox, gt_class和gt_score的各项数值全设置为0\n",
    "    MAX_NUM = 50\n",
    "    gt_bbox2 = np.zeros((MAX_NUM, 4))\n",
    "    gt_class2 = np.zeros((MAX_NUM,))\n",
    "    for i in range(len(gt_bbox)):\n",
    "        gt_bbox2[i, :] = gt_bbox[i, :]\n",
    "        gt_class2[i] = gt_class[i]\n",
    "        if i >= MAX_NUM:\n",
    "            break\n",
    "    return gt_bbox2, gt_class2\n",
    "\n",
    "def get_img_data_from_file(record):\n",
    "    \"\"\"\n",
    "    record is a dict as following,\n",
    "      record = {\n",
    "            'im_file': img_file,\n",
    "            'im_id': im_id,\n",
    "            'h': im_h,\n",
    "            'w': im_w,\n",
    "            'is_crowd': is_crowd,\n",
    "            'gt_class': gt_class,\n",
    "            'gt_bbox': gt_bbox,\n",
    "            'gt_poly': [],\n",
    "            'difficult': difficult\n",
    "            }\n",
    "    \"\"\"\n",
    "    im_file = record['im_file']\n",
    "    h = record['h']\n",
    "    w = record['w']\n",
    "    is_crowd = record['is_crowd']\n",
    "    gt_class = record['gt_class']\n",
    "    gt_bbox = record['gt_bbox']\n",
    "    difficult = record['difficult']\n",
    "\n",
    "    img = cv2.imread(im_file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # check if h and w in record equals that read from img\n",
    "    assert img.shape[0] == int(h), \\\n",
    "             \"image height of {} inconsistent in record({}) and img file({})\".format(\n",
    "               im_file, h, img.shape[0])\n",
    "\n",
    "    assert img.shape[1] == int(w), \\\n",
    "             \"image width of {} inconsistent in record({}) and img file({})\".format(\n",
    "               im_file, w, img.shape[1])\n",
    "\n",
    "    gt_boxes, gt_labels = get_bbox(gt_bbox, gt_class)\n",
    "\n",
    "    # gt_bbox 用相对值\n",
    "    gt_boxes[:, 0] = gt_boxes[:, 0] / float(w)\n",
    "    gt_boxes[:, 1] = gt_boxes[:, 1] / float(h)\n",
    "    gt_boxes[:, 2] = gt_boxes[:, 2] / float(w)\n",
    "    gt_boxes[:, 3] = gt_boxes[:, 3] / float(h)\n",
    "  \n",
    "    return img, gt_boxes, gt_labels, (h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " <font size=3>\n",
    "  \n",
    "`get_img_data_from_file()`函数可以返回图片数据的数据，它们是图像数据img，真实框坐标gt_boxes，真实框包含的物体类别gt_labels，图像尺寸scales。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据预处理\n",
    "<font size=3>\n",
    "\n",
    "在计算机视觉中，通常会对图像做一些随机的变化，产生相似但又不完全相同的样本。主要作用是扩大训练数据集，抑制过拟合，提升模型的泛化能力，常用的方法主要有以下几种：\n",
    "- 随机改变亮暗、对比度和颜色\n",
    "- 随机填充\n",
    "- 随机裁剪\n",
    "- 随机缩放\n",
    "- 随机翻转\n",
    "- 随机打乱真实框排列顺序\n",
    "\n",
    "下面我们分别使用numpy 实现这些数据增强方法。\n",
    "\n",
    "#### **随机改变亮暗、对比度和颜色等**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "import random\n",
    "\n",
    "# 随机改变亮暗、对比度和颜色等\n",
    "def random_distort(img):\n",
    "    # 随机改变亮度\n",
    "    def random_brightness(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Brightness(img).enhance(e)\n",
    "    # 随机改变对比度\n",
    "    def random_contrast(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Contrast(img).enhance(e)\n",
    "    # 随机改变颜色\n",
    "    def random_color(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Color(img).enhance(e)\n",
    "\n",
    "    ops = [random_brightness, random_contrast, random_color]\n",
    "    np.random.shuffle(ops)\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "    img = ops[0](img)\n",
    "    img = ops[1](img)\n",
    "    img = ops[2](img)\n",
    "    img = np.asarray(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "# 定义可视化函数，用于对比原图和图像增强的效果\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize(srcimg, img_enhance):\n",
    "    # 图像可视化\n",
    "    plt.figure(num=2, figsize=(6,12))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Src Image', color='#0000FF')\n",
    "    plt.axis('off') # 不显示坐标轴\n",
    "    plt.imshow(srcimg) # 显示原图片\n",
    "\n",
    "    # 对原图做 随机改变亮暗、对比度和颜色等 数据增强\n",
    "    srcimg_gtbox = records[0]['gt_bbox']\n",
    "    srcimg_label = records[0]['gt_class']\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Enhance Image', color='#0000FF')\n",
    "    plt.axis('off') # 不显示坐标轴\n",
    "    plt.imshow(img_enhance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **随机填充**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机填充\n",
    "def random_expand(img,\n",
    "                  gtboxes,\n",
    "                  max_ratio=4.,\n",
    "                  fill=None,\n",
    "                  keep_ratio=True,\n",
    "                  thresh=0.5):\n",
    "    if random.random() > thresh:\n",
    "        return img, gtboxes\n",
    "\n",
    "    if max_ratio < 1.0:\n",
    "        return img, gtboxes\n",
    "\n",
    "    h, w, c = img.shape\n",
    "    ratio_x = random.uniform(1, max_ratio)\n",
    "    if keep_ratio:\n",
    "        ratio_y = ratio_x\n",
    "    else:\n",
    "        ratio_y = random.uniform(1, max_ratio)\n",
    "    oh = int(h * ratio_y)\n",
    "    ow = int(w * ratio_x)\n",
    "    off_x = random.randint(0, ow - w)\n",
    "    off_y = random.randint(0, oh - h)\n",
    "\n",
    "    out_img = np.zeros((oh, ow, c))\n",
    "    if fill and len(fill) == c:\n",
    "        for i in range(c):\n",
    "            out_img[:, :, i] = fill[i] * 255.0\n",
    "\n",
    "    out_img[off_y:off_y + h, off_x:off_x + w, :] = img\n",
    "    gtboxes[:, 0] = ((gtboxes[:, 0] * w) + off_x) / float(ow)\n",
    "    gtboxes[:, 1] = ((gtboxes[:, 1] * h) + off_y) / float(oh)\n",
    "    gtboxes[:, 2] = gtboxes[:, 2] / ratio_x\n",
    "    gtboxes[:, 3] = gtboxes[:, 3] / ratio_y\n",
    "\n",
    "    return out_img.astype('uint8'), gtboxes\n",
    "\n",
    "\n",
    "# 对原图做 随机改变亮暗、对比度和颜色等 数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **随机裁剪**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "随机裁剪之前需要先定义两个函数，`multi_box_iou_xywh`和`box_crop`这两个函数将被保存在box_utils.py文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multi_box_iou_xywh(box1, box2):\n",
    "    \"\"\"\n",
    "    In this case, box1 or box2 can contain multi boxes.\n",
    "    Only two cases can be processed in this method:\n",
    "       1, box1 and box2 have the same shape, box1.shape == box2.shape\n",
    "       2, either box1 or box2 contains only one box, len(box1) == 1 or len(box2) == 1\n",
    "    If the shape of box1 and box2 does not match, and both of them contain multi boxes, it will be wrong.\n",
    "    \"\"\"\n",
    "    assert box1.shape[-1] == 4, \"Box1 shape[-1] should be 4.\"\n",
    "    assert box2.shape[-1] == 4, \"Box2 shape[-1] should be 4.\"\n",
    "\n",
    "\n",
    "    b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "    b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "    b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "    b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "\n",
    "    inter_x1 = np.maximum(b1_x1, b2_x1)\n",
    "    inter_x2 = np.minimum(b1_x2, b2_x2)\n",
    "    inter_y1 = np.maximum(b1_y1, b2_y1)\n",
    "    inter_y2 = np.minimum(b1_y2, b2_y2)\n",
    "    inter_w = inter_x2 - inter_x1\n",
    "    inter_h = inter_y2 - inter_y1\n",
    "    inter_w = np.clip(inter_w, a_min=0., a_max=None)\n",
    "    inter_h = np.clip(inter_h, a_min=0., a_max=None)\n",
    "\n",
    "    inter_area = inter_w * inter_h\n",
    "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "\n",
    "    return inter_area / (b1_area + b2_area - inter_area)\n",
    "\n",
    "def box_crop(boxes, labels, crop, img_shape):\n",
    "    x, y, w, h = map(float, crop)\n",
    "    im_w, im_h = map(float, img_shape)\n",
    "\n",
    "    boxes = boxes.copy()\n",
    "    boxes[:, 0], boxes[:, 2] = (boxes[:, 0] - boxes[:, 2] / 2) * im_w, (\n",
    "        boxes[:, 0] + boxes[:, 2] / 2) * im_w\n",
    "    boxes[:, 1], boxes[:, 3] = (boxes[:, 1] - boxes[:, 3] / 2) * im_h, (\n",
    "        boxes[:, 1] + boxes[:, 3] / 2) * im_h\n",
    "\n",
    "    crop_box = np.array([x, y, x + w, y + h])\n",
    "    centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n",
    "    mask = np.logical_and(crop_box[:2] <= centers, centers <= crop_box[2:]).all(\n",
    "        axis=1)\n",
    "\n",
    "    boxes[:, :2] = np.maximum(boxes[:, :2], crop_box[:2])\n",
    "    boxes[:, 2:] = np.minimum(boxes[:, 2:], crop_box[2:])\n",
    "    boxes[:, :2] -= crop_box[:2]\n",
    "    boxes[:, 2:] -= crop_box[:2]\n",
    "\n",
    "    mask = np.logical_and(mask, (boxes[:, :2] < boxes[:, 2:]).all(axis=1))\n",
    "    boxes = boxes * np.expand_dims(mask.astype('float32'), axis=1)\n",
    "    labels = labels * mask.astype('float32')\n",
    "    boxes[:, 0], boxes[:, 2] = (boxes[:, 0] + boxes[:, 2]) / 2 / w, (\n",
    "        boxes[:, 2] - boxes[:, 0]) / w\n",
    "    boxes[:, 1], boxes[:, 3] = (boxes[:, 1] + boxes[:, 3]) / 2 / h, (\n",
    "        boxes[:, 3] - boxes[:, 1]) / h\n",
    "\n",
    "    return boxes, labels, mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机裁剪\n",
    "def random_crop(img,\n",
    "                boxes,\n",
    "                labels,\n",
    "                scales=[0.3, 1.0],\n",
    "                max_ratio=2.0,\n",
    "                constraints=None,\n",
    "                max_trial=50):\n",
    "    if len(boxes) == 0:\n",
    "        return img, boxes\n",
    "\n",
    "    if not constraints:\n",
    "        constraints = [(0.1, 1.0), (0.3, 1.0), (0.5, 1.0), (0.7, 1.0),\n",
    "                       (0.9, 1.0), (0.0, 1.0)]\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "    w, h = img.size\n",
    "    crops = [(0, 0, w, h)]\n",
    "    for min_iou, max_iou in constraints:\n",
    "        for _ in range(max_trial):\n",
    "            scale = random.uniform(scales[0], scales[1])\n",
    "            aspect_ratio = random.uniform(max(1 / max_ratio, scale * scale), \\\n",
    "                                          min(max_ratio, 1 / scale / scale))\n",
    "            crop_h = int(h * scale / np.sqrt(aspect_ratio))\n",
    "            crop_w = int(w * scale * np.sqrt(aspect_ratio))\n",
    "            crop_x = random.randrange(w - crop_w)\n",
    "            crop_y = random.randrange(h - crop_h)\n",
    "            crop_box = np.array([[(crop_x + crop_w / 2.0) / w,\n",
    "                                  (crop_y + crop_h / 2.0) / h,\n",
    "                                  crop_w / float(w), crop_h / float(h)]])\n",
    "\n",
    "            iou = multi_box_iou_xywh(crop_box, boxes)\n",
    "            if min_iou <= iou.min() and max_iou >= iou.max():\n",
    "                crops.append((crop_x, crop_y, crop_w, crop_h))\n",
    "                break\n",
    "\n",
    "    while crops:\n",
    "        crop = crops.pop(np.random.randint(0, len(crops)))\n",
    "        crop_boxes, crop_labels, box_num = box_crop(boxes, labels, crop, (w, h))\n",
    "        if box_num < 1:\n",
    "            continue\n",
    "        img = img.crop((crop[0], crop[1], crop[0] + crop[2],\n",
    "                        crop[1] + crop[3])).resize(img.size, Image.LANCZOS)\n",
    "        img = np.asarray(img)\n",
    "        return img, crop_boxes, crop_labels\n",
    "    img = np.asarray(img)\n",
    "    return img, boxes, labels\n",
    "\n",
    "\n",
    "# 对原图做 随机改变亮暗、对比度和颜色等 数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **随机缩放**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机缩放\n",
    "def random_interp(img, size, interp=None):\n",
    "    interp_method = [\n",
    "        cv2.INTER_NEAREST,\n",
    "        cv2.INTER_LINEAR,\n",
    "        cv2.INTER_AREA,\n",
    "        cv2.INTER_CUBIC,\n",
    "        cv2.INTER_LANCZOS4,\n",
    "    ]\n",
    "    if not interp or interp not in interp_method:\n",
    "        interp = interp_method[random.randint(0, len(interp_method) - 1)]\n",
    "    h, w, _ = img.shape\n",
    "    im_scale_x = size / float(w)\n",
    "    im_scale_y = size / float(h)\n",
    "    img = cv2.resize(\n",
    "        img, None, None, fx=im_scale_x, fy=im_scale_y, interpolation=interp)\n",
    "    return img\n",
    "\n",
    "# 对原图做 随机改变亮暗、对比度和颜色等 数据增强\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **随机翻转**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机翻转\n",
    "def random_flip(img, gtboxes, thresh=0.5):\n",
    "    if random.random() > thresh:\n",
    "        img = img[:, ::-1, :]\n",
    "        gtboxes[:, 0] = 1.0 - gtboxes[:, 0]\n",
    "    return img, gtboxes\n",
    "\n",
    "\n",
    "# 对原图做 随机改变亮暗、对比度和颜色等 数据增强\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **随机打乱真实框排列顺序**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机打乱真实框排列顺序\n",
    "def shuffle_gtbox(gtbox, gtlabel):\n",
    "    gt = np.concatenate(\n",
    "        [gtbox, gtlabel[:, np.newaxis]], axis=1)\n",
    "    idx = np.arange(gt.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    gt = gt[idx, :]\n",
    "    return gt[:, :4], gt[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **图像增广方法汇总**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 图像增广方法汇总\n",
    "def image_augment(img, gtboxes, gtlabels, size, means=None):\n",
    "    # 随机改变亮暗、对比度和颜色等\n",
    "    img = random_distort(img)\n",
    "    # 随机填充\n",
    "    img, gtboxes = random_expand(img, gtboxes, fill=means)\n",
    "    # 随机裁剪\n",
    "    img, gtboxes, gtlabels, = random_crop(img, gtboxes, gtlabels)\n",
    "    # 随机缩放\n",
    "    img = random_interp(img, size)\n",
    "    # 随机翻转\n",
    "    img, gtboxes = random_flip(img, gtboxes)\n",
    "    # 随机打乱真实框排列顺序\n",
    "    gtboxes, gtlabels = shuffle_gtbox(gtboxes, gtlabels)\n",
    "\n",
    "    return img.astype('float32'), gtboxes.astype('float32'), gtlabels.astype('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "这里得到的img数据数值需要调整，需要除以255，并且减去均值和方差，再将维度从[H, W, C]调整为[C, H, W]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "将上面的过程整理成一个`get_img_data`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_img_data(record, size=640):\n",
    "    img, gt_boxes, gt_labels, scales = get_img_data_from_file(record)\n",
    "    img, gt_boxes, gt_labels = image_augment(img, gt_boxes, gt_labels, size)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    mean = np.array(mean).reshape((1, 1, -1))\n",
    "    std = np.array(std).reshape((1, 1, -1))\n",
    "    img = (img / 255.0 - mean) / std\n",
    "    img = img.astype('float32').transpose((2, 0, 1))\n",
    "    return img, gt_boxes, gt_labels, scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 使用飞桨高层API快速实现数据增强\n",
    "<font size=3>\n",
    "  \n",
    "上述代码中，我们使用numpy实现了多种数据增强方式。  \n",
    "  同时飞桨也提供了**拿来即用**的数据增强方法，详细可查阅[paddle.vision.transforms](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/ops/yolo_box_cn.html)模块，transforms模块中提供了数十种数据增强方式，包括亮度增强([adjust_brightness](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/transforms/functional/adjust_brightness_cn.html))，对比度增强([adjust_contrast](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/transforms/functional/adjust_contrast.html))，随机裁剪([RandomCrop](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/transforms/transforms/RandomCrop_cn.html))等等。更多的关于高层API的使用方法，请登录飞桨官网。\n",
    "\n",
    "paddle.vision.transforms模块中的数据增强使用方式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 批量数据读取与加速\n",
    "<font size=3>\n",
    "  \n",
    "上面的程序展示了如何读取一张图片的数据并加速，下面的代码实现了批量数据读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取一个批次内样本随机缩放的尺寸\n",
    "def get_img_size(mode):\n",
    "    if (mode == 'train') or (mode == 'valid'):\n",
    "        inds = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "        ii = np.random.choice(inds)\n",
    "        img_size = 320 + ii * 32\n",
    "    else:\n",
    "        img_size = 608\n",
    "    return img_size\n",
    "\n",
    "# 将 list形式的batch数据 转化成多个array构成的tuple\n",
    "def make_array(batch_data):\n",
    "    img_array = np.array([item[0] for item in batch_data], dtype = 'float32')\n",
    "    gt_box_array = np.array([item[1] for item in batch_data], dtype = 'float32')\n",
    "    gt_labels_array = np.array([item[2] for item in batch_data], dtype = 'int32')\n",
    "    img_scale = np.array([item[3] for item in batch_data], dtype='int32')\n",
    "    return img_array, gt_box_array, gt_labels_array, img_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "由于数据预处理耗时较长，可能会成为网络训练速度的瓶颈，所以需要对预处理部分进行优化。  \n",
    "  通过使用飞桨提供的[paddle.io.DataLoader](https://www.paddlepaddle.org.cn/documentation/docs/en/develop/api/paddle/io/DataLoader_en.html) API中的num_workers参数设置进程数量，实现多进程读取数据，具体实现代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "# 定义数据读取类，继承Paddle.io.Dataset\n",
    "class TrainDataset(paddle.io.Dataset):\n",
    "    def  __init__(self, datadir, mode='train'):\n",
    "        self.datadir = datadir\n",
    "        cname2cid = get_insect_names()\n",
    "        self.records = get_annotations(cname2cid, datadir)\n",
    "        self.img_size = 640  #get_img_size(mode)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        # print(\"print: \", record)\n",
    "        img, gt_bbox, gt_labels, im_shape = get_img_data(record, size=self.img_size)\n",
    "\n",
    "        return img, gt_bbox, gt_labels, np.array(im_shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "至此，我们完成了如何查看数据集中的数据、提取数据标注信息、从文件读取图像和标注数据、图像增广、批量读取和加速等过程，通过`paddle.io.Dataset`可以返回img, gt_boxes, gt_labels, im_shape等数据，接下来就可以将它们输入到神经网络，应用到具体算法上了。\n",
    "\n",
    "在开始具体的算法讲解之前，先补充一下读取测试数据的代码。测试数据没有标注信息，也不需要做图像增广，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 将 list形式的batch数据 转化成多个array构成的tuple\n",
    "def make_test_array(batch_data):\n",
    "    img_name_array = np.array([item[0] for item in batch_data])\n",
    "    img_data_array = np.array([item[1] for item in batch_data], dtype = 'float32')\n",
    "    img_scale_array = np.array([item[2] for item in batch_data], dtype='int32')\n",
    "    return img_name_array, img_data_array, img_scale_array\n",
    "\n",
    "# 测试数据读取\n",
    "def test_data_loader(datadir, batch_size= 10, test_image_size=608, mode='test'):\n",
    "    \"\"\"\n",
    "    加载测试用的图片，测试数据没有groundtruth标签\n",
    "    \"\"\"\n",
    "    image_names = os.listdir(datadir)\n",
    "    def reader():\n",
    "        batch_data = []\n",
    "        img_size = test_image_size\n",
    "        for image_name in image_names:\n",
    "            file_path = os.path.join(datadir, image_name)\n",
    "            img = cv2.imread(file_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            H = img.shape[0]\n",
    "            W = img.shape[1]\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "            mean = [0.485, 0.456, 0.406]\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "            mean = np.array(mean).reshape((1, 1, -1))\n",
    "            std = np.array(std).reshape((1, 1, -1))\n",
    "            out_img = (img / 255.0 - mean) / std\n",
    "            out_img = out_img.astype('float32').transpose((2, 0, 1))\n",
    "            img = out_img #np.transpose(out_img, (2,0,1))\n",
    "            im_shape = [H, W]\n",
    "\n",
    "            batch_data.append((image_name.split('.')[0], img, im_shape))\n",
    "            if len(batch_data) == batch_size:\n",
    "                yield make_test_array(batch_data)\n",
    "                batch_data = []\n",
    "        if len(batch_data) > 0:\n",
    "            yield make_test_array(batch_data)\n",
    "\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 单阶段目标检测模型YOLOv3\n",
    "\n",
    "<font size=3>\n",
    "  \n",
    "R-CNN系列算法需要先产生候选区域，再对候选区域做分类和位置坐标的预测，这类算法被称为两阶段目标检测算法。近几年，很多研究人员相继提出一系列单阶段的检测算法，只需要一个网络即可同时产生候选区域并预测出物体的类别和位置坐标。\n",
    "\n",
    "与R-CNN系列算法不同，YOLOv3使用单个网络结构，在产生候选区域的同时即可预测出物体类别和位置，不需要分成两阶段来完成检测任务。另外，YOLOv3算法产生的预测框数目比Faster R-CNN少很多。Faster R-CNN中每个真实框可能对应多个标签为正的候选区域，而YOLOv3里面每个真实框只对应一个正的候选区域。这些特性使得YOLOv3算法具有更快的速度，能到达实时响应的水平。\n",
    "\n",
    "Joseph Redmon等人在2015年提出YOLO（You Only Look Once，YOLO）算法，通常也被称为YOLOv1；2016年，他们对算法进行改进，又提出YOLOv2版本；2018年发展出YOLOv3版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## YOLOv3模型设计思想\n",
    "\n",
    "<font size=3>\n",
    "  \n",
    "YOLOv3算法的基本思想可以分成两部分：\n",
    "\n",
    "* 按一定规则在图片上产生一系列的候选区域，然后根据这些候选区域与图片上物体真实框之间的位置关系对候选区域进行标注。跟真实框足够接近的那些候选区域会被标注为正样本，同时将真实框的位置作为正样本的位置目标。偏离真实框较大的那些候选区域则会被标注为负样本，负样本不需要预测位置或者类别。\n",
    "* 使用卷积神经网络提取图片特征并对候选区域的位置和类别进行预测。这样每个预测框就可以看成是一个样本，根据真实框相对它的位置和类别进行了标注而获得标签值，通过网络模型预测其位置和类别，将网络预测值和标签值进行比较，就可以建立起损失函数。\n",
    "\n",
    "YOLOv3算法训练过程的流程图如 **图8** 所示：\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f2eb2b75bb5a4e518b86a257e0f931de7377dba3bba44d1e846b307036aed41a\" width = \"800\"></center>\n",
    "<center><br>图8：YOLOv3算法训练流程图 </br></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "* **图8** 左边是输入图片，上半部分所示的过程是使用卷积神经网络对图片提取特征，随着网络不断向前传播，特征图的尺寸越来越小，每个像素点会代表更加抽象的特征模式，直到输出特征图，其尺寸减小为原图的$\\frac{1}{32}$。\n",
    "* **图8** 下半部分描述了生成候选区域的过程，首先将原图划分成多个小方块，每个小方块的大小是$32 \\times 32$，然后以每个小方块为中心分别生成一系列锚框，整张图片都会被锚框覆盖到。在每个锚框的基础上产生一个与之对应的预测框，根据锚框和预测框与图片上物体真实框之间的位置关系，对这些预测框进行标注。\n",
    "* 将上方支路中输出的特征图与下方支路中产生的预测框标签建立关联，创建损失函数，开启端到端的训练过程。\n",
    "\n",
    "接下来具体介绍流程中各节点的原理和代码实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 产生候选区域\n",
    "<font size=3>\n",
    "  \n",
    "如何产生候选区域，是检测模型的核心设计方案。目前大多数基于卷积神经网络的模型所采用的方式大体如下：\n",
    "\n",
    "* 按一定的规则在图片上生成一系列位置固定的锚框，将这些锚框看作是可能的候选区域。\n",
    "* 对锚框是否包含目标物体进行预测，如果包含目标物体，还需要预测所包含物体的类别，以及预测框相对于锚框位置需要调整的幅度。\n",
    "\n",
    "\n",
    "### 生成锚框\n",
    "\n",
    "将原始图片划分成$m\\times n$个区域，如下图所示，原始图片高度$H=640$, 宽度$W=480$，如果我们选择小块区域的尺寸为$32 \\times 32$，则$m$和$n$分别为：\n",
    "\n",
    "$$m = \\frac{640}{32} = 20$$\n",
    "\n",
    "$$n = \\frac{480}{32} = 15$$\n",
    "\n",
    "如 **图9** 所示，将原始图像分成了20行15列小方块区域。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2dd1cbeb53644552a8cb38f3f834dbdda5046a489465454d93cdc88d1ce65ca5\" width = \"400\"></center>\n",
    "<center><br>图9：将图片划分成多个32x32的小方块 </br></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "YOLOv3算法会在每个区域的中心，生成一系列锚框。为了展示方便，我们先在图中第十行第四列的小方块位置附近画出生成的锚框，如 **图10** 所示。\n",
    "\n",
    "------\n",
    "**注意：**\n",
    "\n",
    "这里为了跟程序中的编号对应，最上面的行号是第0行，最左边的列号是第0列。\n",
    "\n",
    "------\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6dd42b9138364a379b6231ac2247d3cb449d612e17be4896986bca2703acbb29\" width = \"400\"></center>\n",
    "<center><br>图10：在第10行第4列的小方块区域生成3个锚框 </br></center>\n",
    "<br></br>\n",
    "\n",
    "**图11** 展示在每个区域附近都生成3个锚框，很多锚框堆叠在一起可能不太容易看清楚，但过程跟上面类似，只是需要以每个区域的中心点为中心，分别生成3个锚框。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0880c3b5ec2d40edb476f4fcbadd87aa9f37059cd24d4a1a9d37c627ce5f618a\" width = \"400\"></center>\n",
    "<center><br>图11：在每个小方块区域生成3个锚框 </br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 对候选区域进行标注\n",
    "<font size=3>\n",
    "\n",
    "每个区域可以产生3种不同形状的锚框，每个锚框都是一个可能的候选区域，对这些候选区域我们需要了解如下几件事情：\n",
    "\n",
    "- 锚框是否包含物体，这可以看成是一个二分类问题，使用标签objectness来表示。当锚框包含了物体时，objectness=1，表示预测框属于正类；当锚框不包含物体时，设置objectness=0，表示锚框属于负类。\n",
    "\n",
    "- 如果锚框包含了物体，那么它对应的预测框的中心位置和大小应该是多少，或者说上面计算式中的$t_x, t_y, t_w, t_h$应该是多少，使用location标签。\n",
    "\n",
    "- 如果锚框包含了物体，那么具体类别是什么，这里使用变量label来表示其所属类别的标签。\n",
    "\n",
    "选取任意一个锚框对它进行标注，也就是需要确定其对应的objectness, $(t_x, t_y, t_w, t_h)$和label，下面将分别讲述如何确定这三个标签的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 标注锚框包含物体类别的标签\n",
    "<font size=3>\n",
    "  \n",
    "对于objectness=1的锚框，需要确定其具体类别。正如上面所说，objectness标注为1的锚框，会有一个真实框跟它对应，该锚框所属物体类别，即是其所对应的真实框包含的物体类别。这里使用one-hot向量来表示类别标签label。比如一共有10个分类，而真实框里面包含的物体类别是第2类，则label为$(0,1,0,0,0,0,0,0,0,0)$\n",
    "\n",
    "对上述步骤进行总结，标注的流程如 **图15** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3b914be0c6274916bc7abe4922d4d0fb75be340172764f7096af5be0c2737c57\" width = \"700\"></center>\n",
    "<center><br>图15：标注流程示意图 </br></center>\n",
    "<br></br>\n",
    "\n",
    "通过这种方式，我们在每个小方块区域都生成了一系列的锚框作为候选区域，并且根据图片上真实物体的位置，标注出了每个候选区域对应的objectness标签、位置需要调整的幅度以及包含的物体所属的类别。位置需要调整的幅度由4个变量描述$(t_x, t_y, t_w, t_h)$，objectness标签需要用一个变量描述$obj$，描述所属类别的变量长度等于类别数C。\n",
    "\n",
    "对于每个锚框，模型需要预测输出$(t_x, t_y, t_w, t_h, P_{obj}, P_1, P_2,... , P_C)$，其中$P_{obj}$是锚框是否包含物体的概率，$P_1, P_2,... , P_C$则是锚框包含的物体属于每个类别的概率。接下来让我们一起学习如何通过卷积神经网络输出这样的预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 标注锚框的具体程序\n",
    "<font size=3>\n",
    "  \n",
    "上面描述了如何对预锚框进行标注，但读者可能仍然对里面的细节不太了解，下面将通过具体的程序完成这一步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 标注预测框的objectness\n",
    "def get_objectness_label(img, gt_boxes, gt_labels, iou_threshold = 0.7,\n",
    "                         anchors = [116, 90, 156, 198, 373, 326],\n",
    "                         num_classes=7, downsample=32):\n",
    "    \"\"\"\n",
    "    img 是输入的图像数据，形状是[N, C, H, W]\n",
    "    gt_boxes，真实框，维度是[N, 50, 4]，其中50是真实框数目的上限，当图片中真实框不足50个时，不足部分的坐标全为0\n",
    "              真实框坐标格式是xywh，这里使用相对值\n",
    "    gt_labels，真实框所属类别，维度是[N, 50]\n",
    "    iou_threshold，当预测框与真实框的iou大于iou_threshold时不将其看作是负样本\n",
    "    anchors，锚框可选的尺寸\n",
    "    anchor_masks，通过与anchors一起确定本层级的特征图应该选用多大尺寸的锚框\n",
    "    num_classes，类别数目\n",
    "    downsample，特征图相对于输入网络的图片尺寸变化的比例\n",
    "    \"\"\"\n",
    "\n",
    "    img_shape = img.shape\n",
    "    batchsize = img_shape[0]\n",
    "    num_anchors = len(anchors) // 2\n",
    "    input_h = img_shape[2]\n",
    "    input_w = img_shape[3]\n",
    "    # 将输入图片划分成num_rows x num_cols个小方块区域，每个小方块的边长是 downsample\n",
    "    # 计算一共有多少行小方块\n",
    "    num_rows = input_h // downsample\n",
    "    # 计算一共有多少列小方块\n",
    "    num_cols = input_w // downsample\n",
    "\n",
    "    label_objectness = np.zeros([batchsize, num_anchors, num_rows, num_cols])\n",
    "    label_classification = np.zeros([batchsize, num_anchors, num_classes, num_rows, num_cols])\n",
    "    label_location = np.zeros([batchsize, num_anchors, 4, num_rows, num_cols])\n",
    "\n",
    "    scale_location = np.ones([batchsize, num_anchors, num_rows, num_cols])\n",
    "\n",
    "    # 对batchsize进行循环，依次处理每张图片\n",
    "    for n in range(batchsize):\n",
    "        # 对图片上的真实框进行循环，依次找出跟真实框形状最匹配的锚框\n",
    "        for n_gt in range(len(gt_boxes[n])):\n",
    "            gt = gt_boxes[n][n_gt]\n",
    "            gt_cls = gt_labels[n][n_gt]\n",
    "            gt_center_x = gt[0]\n",
    "            gt_center_y = gt[1]\n",
    "            gt_width = gt[2]\n",
    "            gt_height = gt[3]\n",
    "            if (gt_height < 1e-3) or (gt_height < 1e-3):\n",
    "                continue\n",
    "            i = int(gt_center_y * num_rows)\n",
    "            j = int(gt_center_x * num_cols)\n",
    "            ious = []\n",
    "            for ka in range(num_anchors):\n",
    "                bbox1 = [0., 0., float(gt_width), float(gt_height)]\n",
    "                anchor_w = anchors[ka * 2]\n",
    "                anchor_h = anchors[ka * 2 + 1]\n",
    "                bbox2 = [0., 0., anchor_w/float(input_w), anchor_h/float(input_h)]\n",
    "                # 计算iou\n",
    "                iou = box_iou_xywh(bbox1, bbox2)\n",
    "                ious.append(iou)\n",
    "            ious = np.array(ious)\n",
    "            inds = np.argsort(ious)\n",
    "            k = inds[-1]\n",
    "            label_objectness[n, k, i, j] = 1\n",
    "            c = gt_cls\n",
    "            label_classification[n, k, c, i, j] = 1.\n",
    "\n",
    "            # for those prediction bbox with objectness =1, set label of location\n",
    "            dx_label = gt_center_x * num_cols - j\n",
    "            dy_label = gt_center_y * num_rows - i\n",
    "            dw_label = np.log(gt_width * input_w / anchors[k*2])\n",
    "            dh_label = np.log(gt_height * input_h / anchors[k*2 + 1])\n",
    "            label_location[n, k, 0, i, j] = dx_label\n",
    "            label_location[n, k, 1, i, j] = dy_label\n",
    "            label_location[n, k, 2, i, j] = dw_label\n",
    "            label_location[n, k, 3, i, j] = dh_label\n",
    "            # scale_location用来调节不同尺寸的锚框对损失函数的贡献，作为加权系数和位置损失函数相乘\n",
    "            scale_location[n, k, i, j] = 2.0 - gt_width * gt_height\n",
    "\n",
    "    # 目前根据每张图片上所有出现过的gt box，都标注出了objectness为正的预测框，剩下的预测框则默认objectness为0\n",
    "    # 对于objectness为1的预测框，标出了他们所包含的物体类别，以及位置回归的目标\n",
    "    return label_objectness.astype('float32'), label_location.astype('float32'), label_classification.astype('float32'), \\\n",
    "             scale_location.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算IoU，矩形框的坐标形式为xywh\r\n",
    "def box_iou_xywh(box1, box2):\r\n",
    "    x1min, y1min = box1[0] - box1[2]/2.0, box1[1] - box1[3]/2.0\r\n",
    "    x1max, y1max = box1[0] + box1[2]/2.0, box1[1] + box1[3]/2.0\r\n",
    "    s1 = box1[2] * box1[3]\r\n",
    "\r\n",
    "    x2min, y2min = box2[0] - box2[2]/2.0, box2[1] - box2[3]/2.0\r\n",
    "    x2max, y2max = box2[0] + box2[2]/2.0, box2[1] + box2[3]/2.0\r\n",
    "    s2 = box2[2] * box2[3]\r\n",
    "\r\n",
    "    xmin = np.maximum(x1min, x2min)\r\n",
    "    ymin = np.maximum(y1min, y2min)\r\n",
    "    xmax = np.minimum(x1max, x2max)\r\n",
    "    ymax = np.minimum(y1max, y2max)\r\n",
    "    inter_h = np.maximum(ymax - ymin, 0.)\r\n",
    "    inter_w = np.maximum(xmax - xmin, 0.)\r\n",
    "    intersection = inter_h * inter_w\r\n",
    "\r\n",
    "    union = s1 + s2 - intersection\r\n",
    "    iou = intersection / union\r\n",
    "    return iou "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "上面的程序实现了对锚框进行标注，对于每个真实框，选出了与它形状最匹配的锚框，将其objectness标注为1，并且将$[d_x^*, d_y^*, t_h^*, t_w^*]$作为正样本位置的标签，真实框包含的物体类别作为锚框的类别。而其余的锚框，objectness将被标注为0，无需标注出位置和类别的标签。\n",
    "\n",
    "- 注意：这里还遗留一个小问题，前面我们说了对于与真实框IoU较大的那些锚框，需要将其objectness标注为-1，不参与损失函数的计算。我们先将这个问题放一放，等到后面建立损失函数的时候再补上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 卷积神经网络提取特征\n",
    "<font size=3>\n",
    "  \n",
    "在上一节图像分类的课程中，我们已经学习过了通过卷积神经网络提取图像特征。通过连续使用多层卷积和池化等操作，能得到语义含义更加丰富的特征图。在检测问题中，也使用卷积神经网络逐层提取图像特征，通过最终的输出特征图来表征物体位置和类别等信息。\n",
    "\n",
    "YOLOv3算法使用的骨干网络是Darknet53。Darknet53网络的具体结构如 **图16** 所示，在ImageNet图像分类任务上取得了很好的成绩。在检测任务中，将图中C0后面的平均池化、全连接层和Softmax去掉，保留从输入到C0部分的网络结构，作为检测模型的基础网络结构，也称为骨干网络。YOLOv3模型会在骨干网络的基础上，再添加检测相关的网络模块。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d5cb1e88d3f44259be1427a90ee454a57738ee8083ad40269f5485988526f30d\" width = \"400\"></center>\n",
    "<center><br>图16：Darknet53网络结构 </br></center>\n",
    "<br></br>\n",
    "\n",
    "下面的程序是Darknet53骨干网络的实现代码，这里将上图中C0、C1、C2所表示的输出数据取出，并查看它们的形状分别是，$C0 [1, 1024, 20, 20]$，$C1 [1, 512, 40, 40]$，$C2 [1, 256, 80, 80]$。\n",
    "\n",
    "- 名词解释：特征图的步幅(stride)\n",
    "\n",
    "在提取特征的过程中通常会使用步幅大于1的卷积或者池化，导致后面的特征图尺寸越来越小，特征图的步幅等于输入图片尺寸除以特征图尺寸。例如：C0的尺寸是$20\\times20$，原图尺寸是$640\\times640$，则C0的步幅是$\\frac{640}{20}=32$。同理，C1的步幅是16，C2的步幅是8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ConvBNLayer(paddle.nn.Layer):\n",
    "    def __init__(self, ch_in, ch_out, \n",
    "                 kernel_size=3, stride=1, groups=1,\n",
    "                 padding=0, act=\"leaky\"):\n",
    "        super(ConvBNLayer, self).__init__()\n",
    "    \n",
    "        self.conv = paddle.nn.Conv2D(\n",
    "            in_channels=ch_in,\n",
    "            out_channels=ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=groups,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Normal(0., 0.02)),\n",
    "            bias_attr=False)\n",
    "    \n",
    "        self.batch_norm = paddle.nn.BatchNorm2D(\n",
    "            num_features=ch_out,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Normal(0., 0.02),\n",
    "                regularizer=paddle.regularizer.L2Decay(0.)),\n",
    "            bias_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Constant(0.0),\n",
    "                regularizer=paddle.regularizer.L2Decay(0.)))\n",
    "        self.act = act\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "        out = self.batch_norm(out)\n",
    "        if self.act == 'leaky':\n",
    "            out = F.leaky_relu(x=out, negative_slope=0.1)\n",
    "        return out\n",
    "    \n",
    "class DownSample(paddle.nn.Layer):\n",
    "    # 下采样，图片尺寸减半，具体实现方式是使用stirde=2的卷积\n",
    "    def __init__(self,\n",
    "                 ch_in,\n",
    "                 ch_out,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 padding=1):\n",
    "\n",
    "        super(DownSample, self).__init__()\n",
    "\n",
    "        self.conv_bn_layer = ConvBNLayer(\n",
    "            ch_in=ch_in,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding)\n",
    "        self.ch_out = ch_out\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv_bn_layer(inputs)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    基本残差块的定义，输入x经过两层卷积，然后接第二层卷积的输出和输入x相加\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBNLayer(\n",
    "            ch_in=ch_in,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "            )\n",
    "        self.conv2 = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "            )\n",
    "    def forward(self, inputs):\n",
    "        conv1 = self.conv1(inputs)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        out = paddle.add(x=inputs, y=conv2)\n",
    "        return out\n",
    "\n",
    "     \n",
    "class LayerWarp(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    添加多层残差块，组成Darknet53网络的一个层级\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in, ch_out, count, is_test=True):\n",
    "        super(LayerWarp,self).__init__()\n",
    "\n",
    "        self.basicblock0 = BasicBlock(ch_in,\n",
    "            ch_out)\n",
    "        self.res_out_list = []\n",
    "        for i in range(1, count):\n",
    "            res_out = self.add_sublayer(\"basic_block_%d\" % (i), # 使用add_sublayer添加子层\n",
    "                BasicBlock(ch_out*2,\n",
    "                    ch_out))\n",
    "            self.res_out_list.append(res_out)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        y = self.basicblock0(inputs)\n",
    "        for basic_block_i in self.res_out_list:\n",
    "            y = basic_block_i(y)\n",
    "        return y\n",
    "\n",
    "# DarkNet 每组残差块的个数，来自DarkNet的网络结构图\n",
    "DarkNet_cfg = {53: ([1, 2, 8, 8, 4])}\n",
    "\n",
    "class DarkNet53_conv_body(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(DarkNet53_conv_body, self).__init__()\n",
    "        self.stages = DarkNet_cfg[53]\n",
    "        self.stages = self.stages[0:5]\n",
    "\n",
    "        # 第一层卷积\n",
    "        self.conv0 = ConvBNLayer(\n",
    "            ch_in=3,\n",
    "            ch_out=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "\n",
    "        # 下采样，使用stride=2的卷积来实现\n",
    "        self.downsample0 = DownSample(\n",
    "            ch_in=32,\n",
    "            ch_out=32 * 2)\n",
    "\n",
    "        # 添加各个层级的实现\n",
    "        self.darknet53_conv_block_list = []\n",
    "        self.downsample_list = []\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            conv_block = self.add_sublayer(\n",
    "                \"stage_%d\" % (i),\n",
    "                LayerWarp(32*(2**(i+1)),\n",
    "                32*(2**i),\n",
    "                stage))\n",
    "            self.darknet53_conv_block_list.append(conv_block)\n",
    "        # 两个层级之间使用DownSample将尺寸减半\n",
    "        for i in range(len(self.stages) - 1):\n",
    "            downsample = self.add_sublayer(\n",
    "                \"stage_%d_downsample\" % i,\n",
    "                DownSample(ch_in=32*(2**(i+1)),\n",
    "                    ch_out=32*(2**(i+2))))\n",
    "            self.downsample_list.append(downsample)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.conv0(inputs)\n",
    "        #print(\"conv1:\",out.numpy())\n",
    "        out = self.downsample0(out)\n",
    "        #print(\"dy:\",out.numpy())\n",
    "        blocks = []\n",
    "        for i, conv_block_i in enumerate(self.darknet53_conv_block_list): #依次将各个层级作用在输入上面\n",
    "            out = conv_block_i(out)\n",
    "            blocks.append(out)\n",
    "            if i < len(self.stages) - 1:\n",
    "                out = self.downsample_list[i](out)\n",
    "        return blocks[-1:-4:-1] # 将C0, C1, C2作为返回值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "上面这段示例代码，指定输入数据的形状是$(1, 3, 640, 640)$，则3个层级的输出特征图的形状分别是$C0 (1, 1024, 20, 20)$，$C1 (1, 512, 40, 40)$和$C2 (1, 256, 80, 80)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 根据输出特征图计算预测框位置和类别\n",
    "<font size=3>\n",
    "  \n",
    "YOLOv3中对每个预测框计算逻辑如下：\n",
    "\n",
    "- 预测框是否包含物体。也可理解为objectness=1的概率是多少，可以用网络输出一个实数$x$，可以用$Sigmoid(x)$表示objectness为正的概率$P_{obj}$\n",
    "\n",
    "- 预测物体位置和形状。物体位置和形状$t_x, t_y, t_w, t_h$可以用网络输出4个实数来表示$t_x, t_y, t_w, t_h$\n",
    "\n",
    "- 预测物体类别。预测图像中物体的具体类别是什么，或者说其属于每个类别的概率分别是多少。总的类别数为C，需要预测物体属于每个类别的概率$(P_1, P_2, ..., P_C)$，可以用网络输出C个实数$(x_1, x_2, ..., x_C)$，对每个实数分别求Sigmoid函数，让$P_i = Sigmoid(x_i)$，则可以表示出物体属于每个类别的概率。\n",
    "\n",
    "\n",
    "对于一个预测框，网络需要输出$(5 + C)$个实数来表征它是否包含物体、位置和形状尺寸以及属于每个类别的概率。\n",
    "\n",
    "由于我们在每个小方块区域都生成了K个预测框，则所有预测框一共需要网络输出的预测值数目是：\n",
    "\n",
    "$$[K(5 + C)] \\times m \\times n $$\n",
    "\n",
    "还有更重要的一点是网络输出必须要能区分出小方块区域的位置来，不能直接将特征图连接一个输出大小为$[K(5 + C)] \\times m \\times n$的全连接层。\n",
    "\n",
    "### 建立输出特征图与预测框之间的关联\n",
    "\n",
    "现在观察特征图，经过多次卷积核池化之后，其步幅stride=32，$640 \\times 480$大小的输入图片变成了$20\\times15$的特征图；而小方块区域的数目正好是$20\\times15$，也就是说可以让特征图上每个像素点分别跟原图上一个小方块区域对应。这也是为什么我们最开始将小方块区域的尺寸设置为32的原因，这样可以巧妙的将小方块区域跟特征图上的像素点对应起来，解决了空间位置的对应关系。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/59bd2592dd9f4f4dada8333307198888e667b15969ce434eb52c0232d9608a62\" width = \"600\"></center>\n",
    "<center><br>图17：特征图C0与小方块区域形状对比 </br></center>\n",
    "<br></br>\n",
    "\n",
    "下面需要将像素点$(i,j)$与第i行第j列的小方块区域所需要的预测值关联起来，每个小方块区域产生K个预测框，每个预测框需要$(5 + C)$个实数预测值，则每个像素点相对应的要有$K(5 + C)$个实数。为了解决这一问题，对特征图进行多次卷积，并将最终的输出通道数设置为$K(5 + C)$，即可将生成的特征图与每个预测框所需要的预测值巧妙的对应起来。当然，这种对应是为了将骨干网络提取的特征对接输出层来形成Loss。实际中，这几个尺寸可以随着任务数据分布的不同而调整，只要保证特征图输出尺寸（控制卷积核和下采样）和输出层尺寸（控制小方块区域的大小）相同即可。\n",
    "\n",
    "骨干网络的输出特征图是C0，下面的程序是对C0进行多次卷积以得到跟预测框相关的特征图P0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class YoloDetectionBlock(paddle.nn.Layer):\n",
    "    # define YOLOv3 detection head\n",
    "    # 使用多层卷积和BN提取特征\n",
    "    def __init__(self,ch_in,ch_out,is_test=True):\n",
    "        super(YoloDetectionBlock, self).__init__()\n",
    "\n",
    "        assert ch_out % 2 == 0, \\\n",
    "            \"channel {} cannot be divided by 2\".format(ch_out)\n",
    "\n",
    "        self.conv0 = ConvBNLayer(\n",
    "            ch_in=ch_in,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.conv1 = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "        self.conv2 = ConvBNLayer(\n",
    "            ch_in=ch_out*2,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.conv3 = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "        self.route = ConvBNLayer(\n",
    "            ch_in=ch_out*2,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.tip = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv0(inputs)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        route = self.route(out)\n",
    "        tip = self.tip(route)\n",
    "        return route, tip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "如上面的代码所示，可以由特征图C0生成特征图P0，P0的形状是$[1, 36, 20, 20]$。每个小方块区域生成的锚框或者预测框的数量是3，物体类别数目是7，每个区域需要的预测值个数是$3 \\times (5 + 7) = 36$，正好等于P0的输出通道数。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/9ea44b2c11f74f1484ab2bdc93be4008008cdee0b8d34dcb97bc9f89af935d1c\" width = \"800\"></center>\n",
    "<center><br>图18：特征图P0与候选区域的关联 </br></center>\n",
    "<br></br>\n",
    "\n",
    "将$P0[t, 0:12, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框所需要的12个预测值对应，$P0[t, 12:24, i, j]$与输入的第t张图片上小方块区域$(i, j)$第2个预测框所需要的12个预测值对应，$P0[t, 24:36, i, j]$与输入的第t张图片上小方块区域$(i, j)$第3个预测框所需要的12个预测值对应。\n",
    "\n",
    "$P0[t, 0:4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的位置对应，$P0[t, 4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的objectness对应，$P0[t, 5:12, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的类别对应。\n",
    "\n",
    "如 **图18** 所示，通过这种方式可以巧妙的将网络输出特征图，与每个小方块区域生成的预测框对应起来了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 计算预测框是否包含物体的概率\n",
    "<font size=3>\n",
    "  \n",
    "根据前面的分析，$P0[t, 4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的objectness对应，$P0[t, 4+12, i, j]$与第2个预测框的objectness对应，...，则可以使用下面的程序将objectness相关的预测取出，并使用`paddle.nn.functional.sigmoid`计算输出概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 计算预测框位置坐标\n",
    "<font size=3>\n",
    "  \n",
    "$P0[t, 0:4, i, j]$与输入的第$t$张图片上小方块区域$(i, j)$第1个预测框的位置对应，$P0[t, 12:16, i, j]$与第2个预测框的位置对应，...，使用下面的程序可以从$P0$中取出跟预测框位置相关的预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义Sigmoid函数\n",
    "def sigmoid(x):\n",
    "    return 1./(1.0 + np.exp(-x))\n",
    "\n",
    "# 将网络特征图输出的[tx, ty, th, tw]转化成预测框的坐标[x1, y1, x2, y2]\n",
    "def get_yolo_box_xxyy(pred, anchors, num_classes, downsample):\n",
    "    \"\"\"\n",
    "    pred是网络输出特征图转化成的numpy.ndarray\n",
    "    anchors 是一个list。表示锚框的大小，\n",
    "                例如 anchors = [116, 90, 156, 198, 373, 326]，表示有三个锚框，\n",
    "                第一个锚框大小[w, h]是[116, 90]，第二个锚框大小是[156, 198]，第三个锚框大小是[373, 326]\n",
    "    \"\"\"\n",
    "    batchsize = pred.shape[0]\n",
    "    num_rows = pred.shape[-2]\n",
    "    num_cols = pred.shape[-1]\n",
    "\n",
    "    input_h = num_rows * downsample\n",
    "    input_w = num_cols * downsample\n",
    "\n",
    "    num_anchors = len(anchors) // 2\n",
    "\n",
    "    # pred的形状是[N, C, H, W]，其中C = NUM_ANCHORS * (5 + NUM_CLASSES)\n",
    "    # 对pred进行reshape\n",
    "    pred = pred.reshape([-1, num_anchors, 5+num_classes, num_rows, num_cols])\n",
    "    pred_location = pred[:, :, 0:4, :, :]\n",
    "    pred_location = np.transpose(pred_location, (0,3,4,1,2))\n",
    "    anchors_this = []\n",
    "    for ind in range(num_anchors):\n",
    "        anchors_this.append([anchors[ind*2], anchors[ind*2+1]])\n",
    "    anchors_this = np.array(anchors_this).astype('float32')\n",
    "    \n",
    "    # 最终输出数据保存在pred_box中，其形状是[N, H, W, NUM_ANCHORS, 4]，\n",
    "    # 其中最后一个维度4代表位置的4个坐标\n",
    "    pred_box = np.zeros(pred_location.shape)\n",
    "    for n in range(batchsize):\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                for k in range(num_anchors):\n",
    "                    pred_box[n, i, j, k, 0] = j\n",
    "                    pred_box[n, i, j, k, 1] = i\n",
    "                    pred_box[n, i, j, k, 2] = anchors_this[k][0]\n",
    "                    pred_box[n, i, j, k, 3] = anchors_this[k][1]\n",
    "\n",
    "    # 这里使用相对坐标，pred_box的输出元素数值在0.~1.0之间\n",
    "    pred_box[:, :, :, :, 0] = (sigmoid(pred_location[:, :, :, :, 0]) + pred_box[:, :, :, :, 0]) / num_cols\n",
    "    pred_box[:, :, :, :, 1] = (sigmoid(pred_location[:, :, :, :, 1]) + pred_box[:, :, :, :, 1]) / num_rows\n",
    "    pred_box[:, :, :, :, 2] = np.exp(pred_location[:, :, :, :, 2]) * pred_box[:, :, :, :, 2] / input_w\n",
    "    pred_box[:, :, :, :, 3] = np.exp(pred_location[:, :, :, :, 3]) * pred_box[:, :, :, :, 3] / input_h\n",
    "\n",
    "    # 将坐标从xywh转化成xyxy\n",
    "    pred_box[:, :, :, :, 0] = pred_box[:, :, :, :, 0] - pred_box[:, :, :, :, 2] / 2.\n",
    "    pred_box[:, :, :, :, 1] = pred_box[:, :, :, :, 1] - pred_box[:, :, :, :, 3] / 2.\n",
    "    pred_box[:, :, :, :, 2] = pred_box[:, :, :, :, 0] + pred_box[:, :, :, :, 2]\n",
    "    pred_box[:, :, :, :, 3] = pred_box[:, :, :, :, 1] + pred_box[:, :, :, :, 3]\n",
    "\n",
    "    pred_box = np.clip(pred_box, 0., 1.0)\n",
    "\n",
    "    return pred_box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 损失函数  \n",
    "<font size=3>\n",
    "  \n",
    "上面从概念上将输出特征图上的像素点与预测框关联起来了，那么要对神经网络进行求解，还必须从数学上将网络输出和预测框关联起来，也就是要建立起损失函数跟网络输出之间的关系。下面讨论如何建立起YOLOv3的损失函数。\n",
    "\n",
    "对于每个预测框，YOLOv3模型会建立三种类型的损失函数：\n",
    "\n",
    "- 表征是否包含目标物体的损失函数，通过pred_objectness和label_objectness计算。\n",
    "\n",
    "        loss_obj = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_objectness, label_objectness)\n",
    "\n",
    "- 表征物体位置的损失函数，通过pred_location和label_location计算。\n",
    "\n",
    "        pred_location_x = pred_location[:, :, 0, :, :]\n",
    "        pred_location_y = pred_location[:, :, 1, :, :]\n",
    "        pred_location_w = pred_location[:, :, 2, :, :]\n",
    "        pred_location_h = pred_location[:, :, 3, :, :]\n",
    "        loss_location_x = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_location_x, label_location_x)\n",
    "        loss_location_y = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_location_y, label_location_y)\n",
    "        loss_location_w = paddle.abs(pred_location_w - label_location_w)\n",
    "        loss_location_h = paddle.abs(pred_location_h - label_location_h)\n",
    "        loss_location = loss_location_x + loss_location_y + loss_location_w + loss_location_h\n",
    "\n",
    "- 表征物体类别的损失函数，通过pred_classification和label_classification计算。\n",
    "\n",
    "        loss_obj = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_classification, label_classification)\n",
    "\n",
    "我们已经知道怎么计算这些预测值和标签了，但是遗留了一个小问题，就是没有标注出哪些锚框的objectness为-1。为了完成这一步，我们需要计算出所有预测框跟真实框之间的IoU，然后把那些IoU大于阈值的真实框挑选出来。实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 挑选出跟真实框IoU大于阈值的预测框\n",
    "def get_iou_above_thresh_inds(pred_box, gt_boxes, iou_threshold):\n",
    "    batchsize = pred_box.shape[0]\n",
    "    num_rows = pred_box.shape[1]\n",
    "    num_cols = pred_box.shape[2]\n",
    "    num_anchors = pred_box.shape[3]\n",
    "    ret_inds = np.zeros([batchsize, num_rows, num_cols, num_anchors])\n",
    "    for i in range(batchsize):\n",
    "        pred_box_i = pred_box[i]\n",
    "        gt_boxes_i = gt_boxes[i]\n",
    "        for k in range(len(gt_boxes_i)): #gt in gt_boxes_i:\n",
    "            gt = gt_boxes_i[k]\n",
    "            gtx_min = gt[0] - gt[2] / 2.\n",
    "            gty_min = gt[1] - gt[3] / 2.\n",
    "            gtx_max = gt[0] + gt[2] / 2.\n",
    "            gty_max = gt[1] + gt[3] / 2.\n",
    "            if (gtx_max - gtx_min < 1e-3) or (gty_max - gty_min < 1e-3):\n",
    "                continue\n",
    "            x1 = np.maximum(pred_box_i[:, :, :, 0], gtx_min)\n",
    "            y1 = np.maximum(pred_box_i[:, :, :, 1], gty_min)\n",
    "            x2 = np.minimum(pred_box_i[:, :, :, 2], gtx_max)\n",
    "            y2 = np.minimum(pred_box_i[:, :, :, 3], gty_max)\n",
    "            intersection = np.maximum(x2 - x1, 0.) * np.maximum(y2 - y1, 0.)\n",
    "            s1 = (gty_max - gty_min) * (gtx_max - gtx_min)\n",
    "            s2 = (pred_box_i[:, :, :, 2] - pred_box_i[:, :, :, 0]) * (pred_box_i[:, :, :, 3] - pred_box_i[:, :, :, 1])\n",
    "            union = s2 + s1 - intersection\n",
    "            iou = intersection / union\n",
    "            above_inds = np.where(iou > iou_threshold)\n",
    "            ret_inds[i][above_inds] = 1\n",
    "    ret_inds = np.transpose(ret_inds, (0,3,1,2))\n",
    "    return ret_inds.astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "上面的函数可以得到哪些锚框的objectness需要被标注为-1，通过下面的程序，对label_objectness进行处理，将IoU大于阈值，但又不是正样本的锚框标注为-1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_objectness_ignore(label_objectness, iou_above_thresh_indices):\n",
    "    # 注意：这里不能简单的使用 label_objectness[iou_above_thresh_indices] = -1，\n",
    "    #         这样可能会造成label_objectness为1的点被设置为-1了\n",
    "    #         只有将那些被标注为0，且与真实框IoU超过阈值的预测框才被标注为-1\n",
    "    negative_indices = (label_objectness < 0.5)\n",
    "    ignore_indices = negative_indices * iou_above_thresh_indices\n",
    "    label_objectness[ignore_indices] = -1\n",
    "    return label_objectness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "下面通过调用这两个函数，实现如何将部分预测框的label_objectness设置为-1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_loss(output, label_objectness, label_location, label_classification, scales, num_anchors=3, num_classes=7):\n",
    "    # 将output从[N, C, H, W]变形为[N, NUM_ANCHORS, NUM_CLASSES + 5, H, W]\n",
    "    reshaped_output = paddle.reshape(output, [-1, num_anchors, num_classes + 5, output.shape[2], output.shape[3]])\n",
    "\n",
    "    # 从output中取出跟objectness相关的预测值\n",
    "    pred_objectness = reshaped_output[:, :, 4, :, :]\n",
    "    loss_objectness = F.binary_cross_entropy_with_logits(pred_objectness, label_objectness, reduction=\"none\")\n",
    "\n",
    "    # pos_samples 只有在正样本的地方取值为1.，其它地方取值全为0.\n",
    "    pos_objectness = label_objectness > 0\n",
    "    pos_samples = paddle.cast(pos_objectness, 'float32')\n",
    "    pos_samples.stop_gradient=True\n",
    "\n",
    "    # 从output中取出所有跟位置相关的预测值\n",
    "    tx = reshaped_output[:, :, 0, :, :]\n",
    "    ty = reshaped_output[:, :, 1, :, :]\n",
    "    tw = reshaped_output[:, :, 2, :, :]\n",
    "    th = reshaped_output[:, :, 3, :, :]\n",
    "\n",
    "    # 从label_location中取出各个位置坐标的标签\n",
    "    dx_label = label_location[:, :, 0, :, :]\n",
    "    dy_label = label_location[:, :, 1, :, :]\n",
    "    tw_label = label_location[:, :, 2, :, :]\n",
    "    th_label = label_location[:, :, 3, :, :]\n",
    "\n",
    "    # 构建损失函数\n",
    "    loss_location_x = F.binary_cross_entropy_with_logits(tx, dx_label, reduction=\"none\")\n",
    "    loss_location_y = F.binary_cross_entropy_with_logits(ty, dy_label, reduction=\"none\")\n",
    "    loss_location_w = paddle.abs(tw - tw_label)\n",
    "    loss_location_h = paddle.abs(th - th_label)\n",
    "\n",
    "    # 计算总的位置损失函数\n",
    "    loss_location = loss_location_x + loss_location_y + loss_location_h + loss_location_w\n",
    "\n",
    "    # 乘以scales\n",
    "    loss_location = loss_location * scales\n",
    "    # 只计算正样本的位置损失函数\n",
    "    loss_location = loss_location * pos_samples\n",
    "\n",
    "    # 从output取出所有跟物体类别相关的像素点\n",
    "    pred_classification = reshaped_output[:, :, 5:5+num_classes, :, :]\n",
    "\n",
    "    # 计算分类相关的损失函数\n",
    "    loss_classification = F.binary_cross_entropy_with_logits(pred_classification, label_classification, reduction=\"none\")\n",
    "   \n",
    "    # 将第2维求和\n",
    "    loss_classification = paddle.sum(loss_classification, axis=2)\n",
    "    # 只计算objectness为正的样本的分类损失函数\n",
    "    loss_classification = loss_classification * pos_samples\n",
    "    total_loss = loss_objectness + loss_location + loss_classification\n",
    "    # 对所有预测框的loss进行求和\n",
    "    total_loss = paddle.sum(total_loss, axis=[1,2,3])\n",
    "    # 对所有样本求平均\n",
    "    total_loss = paddle.mean(total_loss)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "上面的程序计算出了总的损失函数，看到这里，读者已经了解到了YOLOv3算法的大部分内容，包括如何生成锚框、给锚框打上标签、通过卷积神经网络提取特征、将输出特征图跟预测框相关联、建立起损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 多尺度检测\n",
    "<font size=3>\n",
    "  \n",
    "目前我们计算损失函数是在特征图P0的基础上进行的，它的步幅stride=32。特征图的尺寸比较小，像素点数目比较少，每个像素点的感受野很大，具有非常丰富的高层级语义信息，可能比较容易检测到较大的目标。为了能够检测到尺寸较小的那些目标，需要在尺寸较大的特征图上面建立预测输出。如果我们在C2或者C1这种层级的特征图上直接产生预测输出，可能面临新的问题，它们没有经过充分的特征提取，像素点包含的语义信息不够丰富，有可能难以提取到有效的特征模式。在目标检测中，解决这一问题的方式是，将高层级的特征图尺寸放大之后跟低层级的特征图进行融合，得到的新特征图既能包含丰富的语义信息，又具有较多的像素点，能够描述更加精细的结构。\n",
    "\n",
    "具体的网络实现方式如 **图19** 所示：\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b6d3b425644342e48bd0a50ebde90d882fd10717e0e44a53a44e98225bbb6df8\" width = \"800\"></center>\n",
    "<center><br>图19：生成多层级的输出特征图P0、P1、P2 </br></center>\n",
    "<br></br>\n",
    "\n",
    "YOLOv3在每个区域的中心位置产生3个锚框，在3个层级的特征图上产生锚框的大小分别为P2 [(10×13),(16×30),(33×23)]，P1 [(30×61),(62×45),(59× 119)]，P0[(116 × 90), (156 × 198), (373 × 326]。越往后的特征图上用到的锚框尺寸也越大，能捕捉到大尺寸目标的信息；越往前的特征图上锚框尺寸越小，能捕捉到小尺寸目标的信息。\n",
    "\n",
    "因为有多尺度的检测，所以需要对上面的代码进行较大的修改，而且实现过程也略显繁琐，所以推荐大家直接使用飞桨 [paddle.vision.ops.yolo_loss](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/ops/yolo_loss_cn.html) API，关键参数说明如下：\n",
    "\n",
    "> paddle.vision.ops.yolo_loss(x, gt_box, gt_label, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, gt_score=None, use_label_smooth=True, name=None, scale_x_y=1.0)\n",
    "\n",
    "- x: 输出特征图。\n",
    "- gt_box: 真实框。\n",
    "- gt_label: 真实框标签。\n",
    "- ignore_thresh，预测框与真实框IoU阈值超过ignore_thresh时，不作为负样本，YOLOv3模型里设置为0.7。\n",
    "- downsample_ratio，特征图P0的下采样比例，使用Darknet53骨干网络时为32。\n",
    "- gt_score，真实框的置信度，在使用了mixup技巧时用到。\n",
    "- use_label_smooth，一种训练技巧，如不使用，设置为False。\n",
    "- name，该层的名字，比如'yolov3_loss'，默认值为None，一般无需设置。\n",
    "\n",
    "对于使用了多层级特征图产生预测框的方法，其具体实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义上采样模块\n",
    "class Upsample(paddle.nn.Layer):\n",
    "    def __init__(self, scale=2):\n",
    "        super(Upsample,self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # get dynamic upsample output shape\n",
    "        shape_nchw = paddle.shape(inputs)\n",
    "        shape_hw = paddle.slice(shape_nchw, axes=[0], starts=[2], ends=[4])\n",
    "        shape_hw.stop_gradient = True\n",
    "        in_shape = paddle.cast(shape_hw, dtype='int32')\n",
    "        out_shape = in_shape * self.scale\n",
    "        out_shape.stop_gradient = True\n",
    "\n",
    "        # reisze by actual_shape\n",
    "        out = paddle.nn.functional.interpolate(\n",
    "            x=inputs, scale_factor=self.scale, mode=\"NEAREST\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class YOLOv3(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(YOLOv3,self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        # 提取图像特征的骨干代码\n",
    "        self.block = DarkNet53_conv_body()\n",
    "        self.block_outputs = []\n",
    "        self.yolo_blocks = []\n",
    "        self.route_blocks_2 = []\n",
    "        # 生成3个层级的特征图P0, P1, P2\n",
    "        for i in range(3):\n",
    "            # 添加从ci生成ri和ti的模块\n",
    "            yolo_block = self.add_sublayer(\n",
    "                \"yolo_detecton_block_%d\" % (i),\n",
    "                YoloDetectionBlock(\n",
    "                                   ch_in=512//(2**i)*2 if i==0 else 512//(2**i)*2 + 512//(2**i),\n",
    "                                   ch_out = 512//(2**i)))\n",
    "            self.yolo_blocks.append(yolo_block)\n",
    "\n",
    "            num_filters = 3 * (self.num_classes + 5)\n",
    "\n",
    "            # 添加从ti生成pi的模块，这是一个Conv2D操作，输出通道数为3 * (num_classes + 5)\n",
    "            block_out = self.add_sublayer(\n",
    "                \"block_out_%d\" % (i),\n",
    "                paddle.nn.Conv2D(in_channels=512//(2**i)*2,\n",
    "                       out_channels=num_filters,\n",
    "                       kernel_size=1,\n",
    "                       stride=1,\n",
    "                       padding=0,\n",
    "                       weight_attr=paddle.ParamAttr(\n",
    "                           initializer=paddle.nn.initializer.Normal(0., 0.02)),\n",
    "                       bias_attr=paddle.ParamAttr(\n",
    "                           initializer=paddle.nn.initializer.Constant(0.0),\n",
    "                           regularizer=paddle.regularizer.L2Decay(0.))))\n",
    "            self.block_outputs.append(block_out)\n",
    "            if i < 2:\n",
    "                # 对ri进行卷积\n",
    "                route = self.add_sublayer(\"route2_%d\"%i,\n",
    "                                          ConvBNLayer(ch_in=512//(2**i),\n",
    "                                                      ch_out=256//(2**i),\n",
    "                                                      kernel_size=1,\n",
    "                                                      stride=1,\n",
    "                                                      padding=0))\n",
    "                self.route_blocks_2.append(route)\n",
    "            # 将ri放大以便跟c_{i+1}保持同样的尺寸\n",
    "            self.upsample = Upsample()\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        blocks = self.block(inputs)\n",
    "        for i, block in enumerate(blocks):\n",
    "            if i > 0:\n",
    "                # 将r_{i-1}经过卷积和上采样之后得到特征图，与这一级的ci进行拼接\n",
    "                block = paddle.concat([route, block], axis=1)\n",
    "            # 从ci生成ti和ri\n",
    "            route, tip = self.yolo_blocks[i](block)\n",
    "            # 从ti生成pi\n",
    "            block_out = self.block_outputs[i](tip)\n",
    "            # 将pi放入列表\n",
    "            outputs.append(block_out)\n",
    "\n",
    "            if i < 2:\n",
    "                # 对ri进行卷积调整通道数\n",
    "                route = self.route_blocks_2[i](route)\n",
    "                # 对ri进行放大，使其尺寸和c_{i+1}保持一致\n",
    "                route = self.upsample(route)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_loss(self, outputs, gtbox, gtlabel, gtscore=None,\n",
    "                 anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326],\n",
    "                 anchor_masks = [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
    "                 ignore_thresh=0.7,\n",
    "                 use_label_smooth=False):\n",
    "        \"\"\"\n",
    "        使用paddle.vision.ops.yolo_loss，直接计算损失函数，过程更简洁，速度也更快\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        downsample = 32\n",
    "        for i, out in enumerate(outputs): # 对三个层级分别求损失函数\n",
    "            anchor_mask_i = anchor_masks[i]\n",
    "            loss = paddle.vision.ops.yolo_loss(\n",
    "                    x=out,  # out是P0, P1, P2中的一个\n",
    "                    gt_box=gtbox,  # 真实框坐标\n",
    "                    gt_label=gtlabel,  # 真实框类别\n",
    "                    gt_score=gtscore,  # 真实框得分，使用mixup训练技巧时需要，不使用该技巧时直接设置为1，形状与gtlabel相同\n",
    "                    anchors=anchors,   # 锚框尺寸，包含[w0, h0, w1, h1, ..., w8, h8]共9个锚框的尺寸\n",
    "                    anchor_mask=anchor_mask_i, # 筛选锚框的mask，例如anchor_mask_i=[3, 4, 5]，将anchors中第3、4、5个锚框挑选出来给该层级使用\n",
    "                    class_num=self.num_classes, # 分类类别数\n",
    "                    ignore_thresh=ignore_thresh, # 当预测框与真实框IoU > ignore_thresh，标注objectness = -1\n",
    "                    downsample_ratio=downsample, # 特征图相对于原图缩小的倍数，例如P0是32， P1是16，P2是8\n",
    "                    use_label_smooth=False)      # 使用label_smooth训练技巧时会用到，这里没用此技巧，直接设置为False\n",
    "            self.losses.append(paddle.mean(loss))  #mean对每张图片求和\n",
    "            downsample = downsample // 2 # 下一级特征图的缩放倍数会减半\n",
    "        return sum(self.losses) # 对每个层级求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 开启端到端训练\n",
    "<font size=3>\n",
    "  \n",
    "训练过程如 **图20** 所示，输入图片经过特征提取得到三个层级的输出特征图P0(stride=32)、P1(stride=16)和P2(stride=8)，相应的分别使用不同大小的小方块区域去生成对应的锚框和预测框，并对这些锚框进行标注。\n",
    "\n",
    "- P0层级特征图，对应着使用$32\\times32$大小的小方块，在每个区域中心生成大小分别为$[116, 90]$, $[156, 198]$, $[373, 326]$的三种锚框。\n",
    "\n",
    "- P1层级特征图，对应着使用$16\\times16$大小的小方块，在每个区域中心生成大小分别为$[30, 61]$, $[62, 45]$, $[59, 119]$的三种锚框。\n",
    "\n",
    "- P2层级特征图，对应着使用$8\\times8$大小的小方块，在每个区域中心生成大小分别为$[10, 13]$, $[16, 30]$, $[33, 23]$的三种锚框。\n",
    "\n",
    "将三个层级的特征图与对应锚框之间的标签关联起来，并建立损失函数，总的损失函数等于三个层级的损失函数相加。通过极小化损失函数，可以开启端到端的训练过程。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/736da9cd3a4f4a1c98187a6cdf1a0334af73470b6d7c4b2fbaa9660d4bd20621\" width = \"600\"></center>\n",
    "<center><br>图20：端到端训练流程 </br></center>\n",
    "<br></br>\n",
    "\n",
    "训练过程的具体实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-26 21:55:15[TRAIN]epoch 0, iter 0, output loss: [17510.273]\n",
      "2021-06-26 21:55:27[TRAIN]epoch 0, iter 10, output loss: [729.24744]\n",
      "2021-06-26 21:55:41[TRAIN]epoch 0, iter 20, output loss: [179.91592]\n",
      "2021-06-26 21:55:54[TRAIN]epoch 0, iter 30, output loss: [86.42448]\n",
      "2021-06-26 21:56:07[TRAIN]epoch 0, iter 40, output loss: [95.52732]\n",
      "2021-06-26 21:56:17[TRAIN]epoch 0, iter 50, output loss: [103.936134]\n",
      "2021-06-26 21:56:30[TRAIN]epoch 0, iter 60, output loss: [97.20747]\n",
      "2021-06-26 21:56:43[TRAIN]epoch 0, iter 70, output loss: [107.33478]\n",
      "2021-06-26 21:56:59[TRAIN]epoch 0, iter 80, output loss: [94.66586]\n",
      "2021-06-26 21:57:12[TRAIN]epoch 0, iter 90, output loss: [106.18151]\n",
      "2021-06-26 21:57:28[TRAIN]epoch 0, iter 100, output loss: [96.802185]\n",
      "2021-06-26 21:57:41[TRAIN]epoch 0, iter 110, output loss: [82.39082]\n",
      "2021-06-26 21:57:55[TRAIN]epoch 0, iter 120, output loss: [44.44758]\n",
      "2021-06-26 21:58:13[TRAIN]epoch 0, iter 130, output loss: [63.05926]\n",
      "2021-06-26 21:58:27[TRAIN]epoch 0, iter 140, output loss: [63.726036]\n",
      "2021-06-26 21:58:39[TRAIN]epoch 0, iter 150, output loss: [73.817505]\n",
      "2021-06-26 21:58:53[TRAIN]epoch 0, iter 160, output loss: [58.81138]\n",
      "2021-06-26 21:59:06[TRAIN]epoch 0, iter 170, output loss: [83.74046]\n",
      "2021-06-26 21:59:20[TRAIN]epoch 0, iter 180, output loss: [63.172356]\n",
      "2021-06-26 21:59:32[TRAIN]epoch 0, iter 190, output loss: [59.170586]\n",
      "2021-06-26 21:59:43[TRAIN]epoch 0, iter 200, output loss: [70.40053]\n",
      "2021-06-26 21:59:57[TRAIN]epoch 0, iter 210, output loss: [60.60277]\n",
      "2021-06-26 22:00:10[TRAIN]epoch 0, iter 220, output loss: [59.044136]\n",
      "2021-06-26 22:00:28[TRAIN]epoch 0, iter 230, output loss: [70.504654]\n",
      "2021-06-26 22:00:41[TRAIN]epoch 0, iter 240, output loss: [60.878914]\n",
      "2021-06-26 22:00:56[TRAIN]epoch 0, iter 250, output loss: [63.208565]\n",
      "2021-06-26 22:01:09[TRAIN]epoch 0, iter 260, output loss: [53.14983]\n",
      "2021-06-26 22:01:21[TRAIN]epoch 0, iter 270, output loss: [68.58199]\n",
      "2021-06-26 22:01:37[TRAIN]epoch 0, iter 280, output loss: [56.36757]\n",
      "2021-06-26 22:01:50[TRAIN]epoch 0, iter 290, output loss: [42.00553]\n",
      "2021-06-26 22:02:03[TRAIN]epoch 0, iter 300, output loss: [48.778786]\n",
      "2021-06-26 22:02:20[TRAIN]epoch 0, iter 310, output loss: [47.875023]\n",
      "2021-06-26 22:02:33[TRAIN]epoch 0, iter 320, output loss: [55.658813]\n",
      "2021-06-26 22:02:44[TRAIN]epoch 0, iter 330, output loss: [56.05859]\n",
      "2021-06-26 22:02:59[TRAIN]epoch 0, iter 340, output loss: [57.134705]\n",
      "2021-06-26 22:03:10[TRAIN]epoch 0, iter 350, output loss: [50.1577]\n",
      "2021-06-26 22:03:25[TRAIN]epoch 0, iter 360, output loss: [65.76583]\n",
      "2021-06-26 22:03:41[TRAIN]epoch 0, iter 370, output loss: [52.935547]\n",
      "2021-06-26 22:03:54[TRAIN]epoch 0, iter 380, output loss: [54.76273]\n",
      "2021-06-26 22:04:06[TRAIN]epoch 0, iter 390, output loss: [46.81684]\n",
      "2021-06-26 22:04:21[TRAIN]epoch 0, iter 400, output loss: [45.05744]\n",
      "2021-06-26 22:04:33[TRAIN]epoch 0, iter 410, output loss: [43.279594]\n",
      "2021-06-26 22:04:45[TRAIN]epoch 0, iter 420, output loss: [57.525974]\n",
      "2021-06-26 22:04:51[VALID]epoch 0, iter 0, output loss: [65.31563]\n",
      "2021-06-26 22:04:53[VALID]epoch 0, iter 1, output loss: [56.39296]\n",
      "2021-06-26 22:04:54[VALID]epoch 0, iter 2, output loss: [70.91133]\n",
      "2021-06-26 22:04:55[VALID]epoch 0, iter 3, output loss: [54.752754]\n",
      "2021-06-26 22:04:56[VALID]epoch 0, iter 4, output loss: [65.0854]\n",
      "2021-06-26 22:04:57[VALID]epoch 0, iter 5, output loss: [63.556885]\n",
      "2021-06-26 22:04:58[VALID]epoch 0, iter 6, output loss: [62.856735]\n",
      "2021-06-26 22:05:00[VALID]epoch 0, iter 7, output loss: [43.20346]\n",
      "2021-06-26 22:05:01[VALID]epoch 0, iter 8, output loss: [73.87293]\n",
      "2021-06-26 22:05:02[VALID]epoch 0, iter 9, output loss: [72.60079]\n",
      "2021-06-26 22:05:03[VALID]epoch 0, iter 10, output loss: [67.06847]\n",
      "2021-06-26 22:05:04[VALID]epoch 0, iter 11, output loss: [52.599457]\n",
      "2021-06-26 22:05:06[VALID]epoch 0, iter 12, output loss: [95.450226]\n",
      "2021-06-26 22:05:07[VALID]epoch 0, iter 13, output loss: [80.76738]\n",
      "2021-06-26 22:05:08[VALID]epoch 0, iter 14, output loss: [78.11456]\n",
      "2021-06-26 22:05:10[VALID]epoch 0, iter 15, output loss: [71.652824]\n",
      "2021-06-26 22:05:11[VALID]epoch 0, iter 16, output loss: [67.87125]\n",
      "2021-06-26 22:05:12[VALID]epoch 0, iter 17, output loss: [72.88877]\n",
      "2021-06-26 22:05:13[VALID]epoch 0, iter 18, output loss: [63.220314]\n",
      "2021-06-26 22:05:14[VALID]epoch 0, iter 19, output loss: [67.9615]\n",
      "2021-06-26 22:05:15[VALID]epoch 0, iter 20, output loss: [65.76298]\n",
      "2021-06-26 22:05:16[VALID]epoch 0, iter 21, output loss: [68.26639]\n",
      "2021-06-26 22:05:18[VALID]epoch 0, iter 22, output loss: [71.556816]\n",
      "2021-06-26 22:05:19[VALID]epoch 0, iter 23, output loss: [45.739296]\n",
      "2021-06-26 22:05:20[VALID]epoch 0, iter 24, output loss: [66.593376]\n",
      "2021-06-26 22:05:21[VALID]epoch 0, iter 25, output loss: [83.319534]\n",
      "2021-06-26 22:05:23[VALID]epoch 0, iter 26, output loss: [68.8452]\n",
      "2021-06-26 22:05:23[VALID]epoch 0, iter 27, output loss: [73.807144]\n",
      "2021-06-26 22:05:25[VALID]epoch 0, iter 28, output loss: [80.06075]\n",
      "2021-06-26 22:05:27[VALID]epoch 0, iter 29, output loss: [59.807842]\n",
      "2021-06-26 22:05:29[VALID]epoch 0, iter 30, output loss: [57.80021]\n",
      "2021-06-26 22:05:31[VALID]epoch 0, iter 31, output loss: [68.03446]\n",
      "2021-06-26 22:05:32[VALID]epoch 0, iter 32, output loss: [75.47266]\n",
      "2021-06-26 22:05:34[VALID]epoch 0, iter 33, output loss: [46.72795]\n",
      "2021-06-26 22:05:35[VALID]epoch 0, iter 34, output loss: [71.5379]\n",
      "2021-06-26 22:05:36[VALID]epoch 0, iter 35, output loss: [64.81765]\n",
      "2021-06-26 22:05:37[VALID]epoch 0, iter 36, output loss: [68.20732]\n",
      "2021-06-26 22:05:38[VALID]epoch 0, iter 37, output loss: [85.66239]\n",
      "2021-06-26 22:05:40[VALID]epoch 0, iter 38, output loss: [78.30882]\n",
      "2021-06-26 22:05:40[VALID]epoch 0, iter 39, output loss: [65.24453]\n",
      "2021-06-26 22:05:41[VALID]epoch 0, iter 40, output loss: [58.18512]\n",
      "2021-06-26 22:05:44[VALID]epoch 0, iter 41, output loss: [56.300213]\n",
      "2021-06-26 22:05:46[VALID]epoch 0, iter 42, output loss: [59.21524]\n",
      "2021-06-26 22:05:47[VALID]epoch 0, iter 43, output loss: [54.705303]\n",
      "2021-06-26 22:05:48[VALID]epoch 0, iter 44, output loss: [65.37149]\n",
      "2021-06-26 22:05:49[VALID]epoch 0, iter 45, output loss: [52.86475]\n",
      "2021-06-26 22:05:51[VALID]epoch 0, iter 46, output loss: [43.22911]\n",
      "2021-06-26 22:05:52[VALID]epoch 0, iter 47, output loss: [70.251236]\n",
      "2021-06-26 22:05:54[VALID]epoch 0, iter 48, output loss: [91.16923]\n",
      "2021-06-26 22:05:55[VALID]epoch 0, iter 49, output loss: [49.566193]\n",
      "2021-06-26 22:05:56[VALID]epoch 0, iter 50, output loss: [67.887276]\n",
      "2021-06-26 22:05:57[VALID]epoch 0, iter 51, output loss: [61.722153]\n",
      "2021-06-26 22:05:58[VALID]epoch 0, iter 52, output loss: [65.18132]\n",
      "2021-06-26 22:06:01[VALID]epoch 0, iter 53, output loss: [63.687943]\n",
      "2021-06-26 22:06:04[VALID]epoch 0, iter 54, output loss: [70.03024]\n",
      "2021-06-26 22:06:05[VALID]epoch 0, iter 55, output loss: [75.153015]\n",
      "2021-06-26 22:06:06[VALID]epoch 0, iter 56, output loss: [61.259697]\n",
      "2021-06-26 22:06:06[VALID]epoch 0, iter 57, output loss: [58.50939]\n",
      "2021-06-26 22:06:07[VALID]epoch 0, iter 58, output loss: [72.98577]\n",
      "2021-06-26 22:06:08[VALID]epoch 0, iter 59, output loss: [68.22452]\n",
      "2021-06-26 22:06:10[VALID]epoch 0, iter 60, output loss: [81.48711]\n",
      "2021-06-26 22:06:10[VALID]epoch 0, iter 61, output loss: [80.611046]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "ANCHORS = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n",
    "\n",
    "ANCHOR_MASKS = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "\n",
    "IGNORE_THRESH = .7\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "def get_lr(base_lr = 0.0001, lr_decay = 0.1):\n",
    "    bd = [10000, 20000]\n",
    "    lr = [base_lr, base_lr * lr_decay, base_lr * lr_decay * lr_decay]\n",
    "    learning_rate = paddle.optimizer.lr.PiecewiseDecay(boundaries=bd, values=lr)\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    TRAINDIR = '/home/aistudio/work/insects/train'\n",
    "    TESTDIR = '/home/aistudio/work/insects/test'\n",
    "    VALIDDIR = '/home/aistudio/work/insects/val'\n",
    "    paddle.set_device(\"gpu:0\")\n",
    "    # 创建数据读取类\n",
    "    train_dataset = TrainDataset(TRAINDIR, mode='train')\n",
    "    valid_dataset = TrainDataset(VALIDDIR, mode='valid')\n",
    "    test_dataset = TrainDataset(VALIDDIR, mode='valid')\n",
    "    # 使用paddle.io.DataLoader创建数据读取器，并设置batchsize，进程数量num_workers等参数\n",
    "    train_loader = paddle.io.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True, use_shared_memory=False)\n",
    "    valid_loader = paddle.io.DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0, drop_last=False, use_shared_memory=False)\n",
    "    model = YOLOv3(num_classes = NUM_CLASSES)  #创建模型\n",
    "    learning_rate = get_lr()\n",
    "    opt = paddle.optimizer.Momentum(\n",
    "                 learning_rate=learning_rate,\n",
    "                 momentum=0.9,\n",
    "                 weight_decay=paddle.regularizer.L2Decay(0.0005),\n",
    "                 parameters=model.parameters())  #创建优化器\n",
    "    # opt = paddle.optimizer.Adam(learning_rate=learning_rate, weight_decay=paddle.regularizer.L2Decay(0.0005), parameters=model.parameters())\n",
    "\n",
    "    MAX_EPOCH = 1\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for i, data in enumerate(train_loader()):\n",
    "            img, gt_boxes, gt_labels, img_scale = data\n",
    "            gt_scores = np.ones(gt_labels.shape).astype('float32')\n",
    "            gt_scores = paddle.to_tensor(gt_scores)\n",
    "            img = paddle.to_tensor(img)\n",
    "            gt_boxes = paddle.to_tensor(gt_boxes)\n",
    "            gt_labels = paddle.to_tensor(gt_labels)\n",
    "            outputs = model(img)  #前向传播，输出[P0, P1, P2]\n",
    "            loss = model.get_loss(outputs, gt_boxes, gt_labels, gtscore=gt_scores,\n",
    "                                  anchors = ANCHORS,\n",
    "                                  anchor_masks = ANCHOR_MASKS,\n",
    "                                  ignore_thresh=IGNORE_THRESH,\n",
    "                                  use_label_smooth=False)        # 计算损失函数\n",
    "\n",
    "            loss.backward()    # 反向传播计算梯度\n",
    "            opt.step()  # 更新参数\n",
    "            opt.clear_grad()\n",
    "            if i % 10 == 0:\n",
    "                timestring = time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(time.time()))\n",
    "                print('{}[TRAIN]epoch {}, iter {}, output loss: {}'.format(timestring, epoch, i, loss.numpy()))\n",
    "\n",
    "        # save params of model\n",
    "        if (epoch % 5 == 0) or (epoch == MAX_EPOCH -1):\n",
    "            paddle.save(model.state_dict(), 'yolo_epoch{}'.format(epoch))\n",
    "\n",
    "        # 每个epoch结束之后在验证集上进行测试\n",
    "        model.eval()\n",
    "        for i, data in enumerate(valid_loader()):\n",
    "            img, gt_boxes, gt_labels, img_scale = data\n",
    "            gt_scores = np.ones(gt_labels.shape).astype('float32')\n",
    "            gt_scores = paddle.to_tensor(gt_scores)\n",
    "            img = paddle.to_tensor(img)\n",
    "            gt_boxes = paddle.to_tensor(gt_boxes)\n",
    "            gt_labels = paddle.to_tensor(gt_labels)\n",
    "            outputs = model(img)\n",
    "            loss = model.get_loss(outputs, gt_boxes, gt_labels, gtscore=gt_scores,\n",
    "                                  anchors = ANCHORS,\n",
    "                                  anchor_masks = ANCHOR_MASKS,\n",
    "                                  ignore_thresh=IGNORE_THRESH,\n",
    "                                  use_label_smooth=False)\n",
    "            if i % 1 == 0:\n",
    "                timestring = time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(time.time()))\n",
    "                print('{}[VALID]epoch {}, iter {}, output loss: {}'.format(timestring, epoch, i, loss.numpy()))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测\n",
    "<font size=3>\n",
    "  \n",
    "预测过程流程 **图21** 如下所示：\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/15c140b1844d419cbe237b1a70f4099266aa168c05dc413f8e232f688050fa75\" width = \"400\"></center>\n",
    "<center><br>图21：预测流程 </br></center>\n",
    "<br></br>\n",
    "\n",
    "预测过程可以分为两步：\n",
    "\n",
    "1. 通过网络输出计算出预测框位置和所属类别的得分。 \n",
    "1. 使用非极大值抑制来消除重叠较大的预测框。\n",
    "\n",
    "\n",
    "对于第1步，前面我们已经讲过如何通过网络输出值计算pred_objectness_probability, pred_boxes以及pred_classification_probability，这里推荐大家直接使用[paddle.vision.ops.yolo_box](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/ops/yolo_box_cn.html)，关键参数含义如下：\n",
    "\n",
    "> paddle.vision.ops.yolo_box(x, img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox=True, name=None, scale_x_y=1.0)\n",
    "\n",
    "- x，网络输出特征图，例如上面提到的P0或者P1、P2。\n",
    "- img_size，输入图片尺寸。\n",
    "- anchors，使用到的anchor的尺寸，如[10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n",
    "- anchor_mask: 每个层级上使用的anchor的掩码，[[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "- class_num，物体类别数。\n",
    "- conf_thresh, 置信度阈值，得分低于该阈值的预测框位置数值不用计算直接设置为0.0。\n",
    "- downsample_ratio, 特征图的下采样比例，例如P0是32，P1是16，P2是8。\n",
    "- name=None，名字，例如'yolo_box'，一般无需设置，默认值为None。\n",
    "   \n",
    "返回值包括两项，boxes和scores，其中boxes是所有预测框的坐标值，scores是所有预测框的得分。\n",
    "\n",
    "预测框得分的定义是所属类别的概率乘以其预测框是否包含目标物体的objectness概率，即\n",
    "\n",
    "$$score = P_{obj} \\cdot P_{classification}$$\n",
    "\n",
    "在上面定义的类YOLOv3下面添加函数，get_pred，通过调用`paddle.vision.ops.yolo_box`获得P0、P1、P2三个层级的特征图对应的预测框和得分，并将他们拼接在一块，即可得到所有的预测框及其属于各个类别的得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义YOLOv3模型\n",
    "class YOLOv3(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(YOLOv3,self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        # 提取图像特征的骨干代码\n",
    "        self.block = DarkNet53_conv_body()\n",
    "        self.block_outputs = []\n",
    "        self.yolo_blocks = []\n",
    "        self.route_blocks_2 = []\n",
    "        # 生成3个层级的特征图P0, P1, P2\n",
    "        for i in range(3):\n",
    "            # 添加从ci生成ri和ti的模块\n",
    "            yolo_block = self.add_sublayer(\n",
    "                \"yolo_detecton_block_%d\" % (i),\n",
    "                YoloDetectionBlock(\n",
    "                                   ch_in=512//(2**i)*2 if i==0 else 512//(2**i)*2 + 512//(2**i),\n",
    "                                   ch_out = 512//(2**i)))\n",
    "            self.yolo_blocks.append(yolo_block)\n",
    "\n",
    "            num_filters = 3 * (self.num_classes + 5)\n",
    "\n",
    "            # 添加从ti生成pi的模块，这是一个Conv2D操作，输出通道数为3 * (num_classes + 5)\n",
    "            block_out = self.add_sublayer(\n",
    "                \"block_out_%d\" % (i),\n",
    "                paddle.nn.Conv2D(in_channels=512//(2**i)*2,\n",
    "                       out_channels=num_filters,\n",
    "                       kernel_size=1,\n",
    "                       stride=1,\n",
    "                       padding=0,\n",
    "                       weight_attr=paddle.ParamAttr(\n",
    "                           initializer=paddle.nn.initializer.Normal(0., 0.02)),\n",
    "                       bias_attr=paddle.ParamAttr(\n",
    "                           initializer=paddle.nn.initializer.Constant(0.0),\n",
    "                           regularizer=paddle.regularizer.L2Decay(0.))))\n",
    "            self.block_outputs.append(block_out)\n",
    "            if i < 2:\n",
    "                # 对ri进行卷积\n",
    "                route = self.add_sublayer(\"route2_%d\"%i,\n",
    "                                          ConvBNLayer(ch_in=512//(2**i),\n",
    "                                                      ch_out=256//(2**i),\n",
    "                                                      kernel_size=1,\n",
    "                                                      stride=1,\n",
    "                                                      padding=0))\n",
    "                self.route_blocks_2.append(route)\n",
    "            # 将ri放大以便跟c_{i+1}保持同样的尺寸\n",
    "            self.upsample = Upsample()\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        blocks = self.block(inputs)\n",
    "        for i, block in enumerate(blocks):\n",
    "            if i > 0:\n",
    "                # 将r_{i-1}经过卷积和上采样之后得到特征图，与这一级的ci进行拼接\n",
    "                block = paddle.concat([route, block], axis=1)\n",
    "            # 从ci生成ti和ri\n",
    "            route, tip = self.yolo_blocks[i](block)\n",
    "            # 从ti生成pi\n",
    "            block_out = self.block_outputs[i](tip)\n",
    "            # 将pi放入列表\n",
    "            outputs.append(block_out)\n",
    "\n",
    "            if i < 2:\n",
    "                # 对ri进行卷积调整通道数\n",
    "                route = self.route_blocks_2[i](route)\n",
    "                # 对ri进行放大，使其尺寸和c_{i+1}保持一致\n",
    "                route = self.upsample(route)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_loss(self, outputs, gtbox, gtlabel, gtscore=None,\n",
    "                 anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326],\n",
    "                 anchor_masks = [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
    "                 ignore_thresh=0.7,\n",
    "                 use_label_smooth=False):\n",
    "        \"\"\"\n",
    "        使用paddle.vision.ops.yolo_loss，直接计算损失函数，过程更简洁，速度也更快\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        downsample = 32\n",
    "        for i, out in enumerate(outputs): # 对三个层级分别求损失函数\n",
    "            anchor_mask_i = anchor_masks[i]\n",
    "            loss = paddle.vision.ops.yolo_loss(\n",
    "                    x=out,  # out是P0, P1, P2中的一个\n",
    "                    gt_box=gtbox,  # 真实框坐标\n",
    "                    gt_label=gtlabel,  # 真实框类别\n",
    "                    gt_score=gtscore,  # 真实框得分，使用mixup训练技巧时需要，不使用该技巧时直接设置为1，形状与gtlabel相同\n",
    "                    anchors=anchors,   # 锚框尺寸，包含[w0, h0, w1, h1, ..., w8, h8]共9个锚框的尺寸\n",
    "                    anchor_mask=anchor_mask_i, # 筛选锚框的mask，例如anchor_mask_i=[3, 4, 5]，将anchors中第3、4、5个锚框挑选出来给该层级使用\n",
    "                    class_num=self.num_classes, # 分类类别数\n",
    "                    ignore_thresh=ignore_thresh, # 当预测框与真实框IoU > ignore_thresh，标注objectness = -1\n",
    "                    downsample_ratio=downsample, # 特征图相对于原图缩小的倍数，例如P0是32， P1是16，P2是8\n",
    "                    use_label_smooth=False)      # 使用label_smooth训练技巧时会用到，这里没用此技巧，直接设置为False\n",
    "            self.losses.append(paddle.mean(loss))  #mean对每张图片求和\n",
    "            downsample = downsample // 2 # 下一级特征图的缩放倍数会减半\n",
    "        return sum(self.losses) # 对每个层级求和\n",
    "\n",
    "    def get_pred(self,\n",
    "                 outputs,\n",
    "                 im_shape=None,\n",
    "                 anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326],\n",
    "                 anchor_masks = [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
    "                 valid_thresh = 0.01):\n",
    "        downsample = 32\n",
    "        total_boxes = []\n",
    "        total_scores = []\n",
    "        for i, out in enumerate(outputs):\n",
    "            anchor_mask = anchor_masks[i]\n",
    "            anchors_this_level = []\n",
    "            for m in anchor_mask:\n",
    "                anchors_this_level.append(anchors[2 * m])\n",
    "                anchors_this_level.append(anchors[2 * m + 1])\n",
    "\n",
    "            boxes, scores = paddle.vision.ops.yolo_box(\n",
    "                   x=out,\n",
    "                   img_size=im_shape,\n",
    "                   anchors=anchors_this_level,\n",
    "                   class_num=self.num_classes,\n",
    "                   conf_thresh=valid_thresh,\n",
    "                   downsample_ratio=downsample,\n",
    "                   name=\"yolo_box\" + str(i))\n",
    "            total_boxes.append(boxes)\n",
    "            total_scores.append(\n",
    "                        paddle.transpose(\n",
    "                        scores, perm=[0, 2, 1]))\n",
    "            downsample = downsample // 2\n",
    "\n",
    "        yolo_boxes = paddle.concat(total_boxes, axis=1)\n",
    "        yolo_scores = paddle.concat(total_scores, axis=2)\n",
    "        return yolo_boxes, yolo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "第1步的计算结果会在每个小方块区域都会产生多个预测框，输出预测框中会有很多重合度比较大，需要消除重叠较大的冗余预测框。\n",
    "\n",
    "下面示例代码中的预测框是使用模型对图片预测之后输出的，这里一共选出了11个预测框，在图上画出预测框如下所示。在每个人像周围，都出现了多个预测框，需要消除冗余的预测框以得到最终的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 画图展示目标物体边界框\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.image import imread\n",
    "import math\n",
    "\n",
    "# 定义画矩形框的程序    \n",
    "def draw_rectangle(currentAxis, bbox, edgecolor = 'k', facecolor = 'y', fill=False, linestyle='-'):\n",
    "    # currentAxis，坐标轴，通过plt.gca()获取\n",
    "    # bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]\n",
    "    # edgecolor，边框线条颜色\n",
    "    # facecolor，填充颜色\n",
    "    # fill, 是否填充\n",
    "    # linestype，边框线型\n",
    "    # patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数\n",
    "    rect=patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1, linewidth=1,\n",
    "                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)\n",
    "    currentAxis.add_patch(rect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "这里使用非极大值抑制（non-maximum suppression, nms）来消除冗余框。基本思想是，如果有多个预测框都对应同一个物体，则只选出得分最高的那个预测框，剩下的预测框被丢弃掉。\n",
    "\n",
    "如何判断两个预测框对应的是同一个物体呢，标准该怎么设置？\n",
    "\n",
    "如果两个预测框的类别一样，而且他们的位置重合度比较大，则可以认为他们是在预测同一个目标。非极大值抑制的做法是，选出某个类别得分最高的预测框，然后看哪些预测框跟它的IoU大于阈值，就把这些预测框给丢弃掉。这里IoU的阈值是超参数，需要提前设置，YOLOv3模型里面设置的是0.5。\n",
    "\n",
    "比如在上面的程序中，boxes里面一共对应11个预测框，scores给出了它们预测\"人\"这一类别的得分。\n",
    "\n",
    "- Step0：创建选中列表，keep_list = []\n",
    "- Step1：对得分进行排序，remain_list = [ 3,  5, 10,  2,  9,  0,  1,  6,  4,  7,  8]， \n",
    "- Step2：选出boxes[3]，此时keep_list为空，不需要计算IoU，直接将其放入keep_list，keep_list = [3]， remain_list=[5, 10,  2,  9,  0,  1,  6,  4,  7,  8]\n",
    "- Step3：选出boxes[5]，此时keep_list中已经存在boxes[3]，计算出IoU(boxes[3], boxes[5]) = 0.0，显然小于阈值，则keep_list=[3, 5], remain_list = [10,  2,  9,  0,  1,  6,  4,  7,  8]\n",
    "- Step4：选出boxes[10]，此时keep_list=[3, 5]，计算IoU(boxes[3], boxes[10])=0.0268，IoU(boxes[5], boxes[10])=0.0268 = 0.24，都小于阈值，则keep_list = [3, 5, 10]，remain_list=[2,  9,  0,  1,  6,  4,  7,  8]\n",
    "- Step5：选出boxes[2]，此时keep_list = [3, 5, 10]，计算IoU(boxes[3], boxes[2]) = 0.88，超过了阈值，直接将boxes[2]丢弃，keep_list=[3, 5, 10]，remain_list=[9,  0,  1,  6,  4,  7,  8]\n",
    "- Step6：选出boxes[9]，此时keep_list = [3, 5, 10]，计算IoU(boxes[3], boxes[9]) = 0.0577，IoU(boxes[5], boxes[9]) = 0.205，IoU(boxes[10], boxes[9]) = 0.88，超过了阈值，将boxes[9]丢弃掉。keep_list=[3, 5, 10]，remain_list=[0,  1,  6,  4,  7,  8]\n",
    "- Step7：重复上述Step6直到remain_list为空。\n",
    "\n",
    "最终得到keep_list=[3, 5, 10]，也就是预测框3、5、10被最终挑选出来了，如下图所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 画图展示目标物体边界框\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.image import imread\n",
    "import math\n",
    "\n",
    "# 定义画矩形框的程序    \n",
    "def draw_rectangle(currentAxis, bbox, edgecolor = 'k', facecolor = 'y', fill=False, linestyle='-'):\n",
    "    # currentAxis，坐标轴，通过plt.gca()获取\n",
    "    # bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]\n",
    "    # edgecolor，边框线条颜色\n",
    "    # facecolor，填充颜色\n",
    "    # fill, 是否填充\n",
    "    # linestype，边框线型\n",
    "    # patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数\n",
    "    rect=patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1, linewidth=1,\n",
    "                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)\n",
    "    currentAxis.add_patch(rect)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "非极大值抑制的具体实现代码如下面的`nms`函数的定义，需要说明的是数据集中含有多个类别的物体，所以这里需要做多分类非极大值抑制，其实现原理与非极大值抑制相同，区别在于需要对每个类别都做非极大值抑制，实现代码如下面的`multiclass_nms`所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 非极大值抑制\n",
    "def nms(bboxes, scores, score_thresh, nms_thresh, pre_nms_topk, i=0, c=0):\n",
    "    \"\"\"\n",
    "    nms\n",
    "    \"\"\"\n",
    "    inds = np.argsort(scores)\n",
    "    inds = inds[::-1]\n",
    "    keep_inds = []\n",
    "    while(len(inds) > 0):\n",
    "        cur_ind = inds[0]\n",
    "        cur_score = scores[cur_ind]\n",
    "        # if score of the box is less than score_thresh, just drop it\n",
    "        if cur_score < score_thresh:\n",
    "            break\n",
    "\n",
    "        keep = True\n",
    "        for ind in keep_inds:\n",
    "            current_box = bboxes[cur_ind]\n",
    "            remain_box = bboxes[ind]\n",
    "            iou = box_iou_xyxy(current_box, remain_box)\n",
    "            if iou > nms_thresh:\n",
    "                keep = False\n",
    "                break\n",
    "        if i == 0 and c == 4 and cur_ind == 951:\n",
    "            print('suppressed, ', keep, i, c, cur_ind, ind, iou)\n",
    "        if keep:\n",
    "            keep_inds.append(cur_ind)\n",
    "        inds = inds[1:]\n",
    "\n",
    "    return np.array(keep_inds)\n",
    "\n",
    "# 多分类非极大值抑制\n",
    "def multiclass_nms(bboxes, scores, score_thresh=0.01, nms_thresh=0.45, pre_nms_topk=1000, pos_nms_topk=100):\n",
    "    \"\"\"\n",
    "    This is for multiclass_nms\n",
    "    \"\"\"\n",
    "    batch_size = bboxes.shape[0]\n",
    "    class_num = scores.shape[1]\n",
    "    rets = []\n",
    "    for i in range(batch_size):\n",
    "        bboxes_i = bboxes[i]\n",
    "        scores_i = scores[i]\n",
    "        ret = []\n",
    "        for c in range(class_num):\n",
    "            scores_i_c = scores_i[c]\n",
    "            keep_inds = nms(bboxes_i, scores_i_c, score_thresh, nms_thresh, pre_nms_topk, i=i, c=c)\n",
    "            if len(keep_inds) < 1:\n",
    "                continue\n",
    "            keep_bboxes = bboxes_i[keep_inds]\n",
    "            keep_scores = scores_i_c[keep_inds]\n",
    "            keep_results = np.zeros([keep_scores.shape[0], 6])\n",
    "            keep_results[:, 0] = c\n",
    "            keep_results[:, 1] = keep_scores[:]\n",
    "            keep_results[:, 2:6] = keep_bboxes[:, :]\n",
    "            ret.append(keep_results)\n",
    "        if len(ret) < 1:\n",
    "            rets.append(ret)\n",
    "            continue\n",
    "        ret_i = np.concatenate(ret, axis=0)\n",
    "        scores_i = ret_i[:, 1]\n",
    "        if len(scores_i) > pos_nms_topk:\n",
    "            inds = np.argsort(scores_i)[::-1]\n",
    "            inds = inds[:pos_nms_topk]\n",
    "            ret_i = ret_i[inds]\n",
    "\n",
    "        rets.append(ret_i)\n",
    "\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "下面是完整的测试程序，在测试数据集上的输出结果将会被保存在pred_results.json文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算IoU，矩形框的坐标形式为xyxy，这个函数会被保存在box_utils.py文件中\r\n",
    "def box_iou_xyxy(box1, box2):\r\n",
    "    # 获取box1左上角和右下角的坐标\r\n",
    "    x1min, y1min, x1max, y1max = box1[0], box1[1], box1[2], box1[3]\r\n",
    "    # 计算box1的面积\r\n",
    "    s1 = (y1max - y1min + 1.) * (x1max - x1min + 1.)\r\n",
    "    # 获取box2左上角和右下角的坐标\r\n",
    "    x2min, y2min, x2max, y2max = box2[0], box2[1], box2[2], box2[3]\r\n",
    "    # 计算box2的面积\r\n",
    "    s2 = (y2max - y2min + 1.) * (x2max - x2min + 1.)\r\n",
    "    \r\n",
    "    # 计算相交矩形框的坐标\r\n",
    "    xmin = np.maximum(x1min, x2min)\r\n",
    "    ymin = np.maximum(y1min, y2min)\r\n",
    "    xmax = np.minimum(x1max, x2max)\r\n",
    "    ymax = np.minimum(y1max, y2max)\r\n",
    "    # 计算相交矩形行的高度、宽度、面积\r\n",
    "    inter_h = np.maximum(ymax - ymin + 1., 0.)\r\n",
    "    inter_w = np.maximum(xmax - xmin + 1., 0.)\r\n",
    "    intersection = inter_h * inter_w\r\n",
    "    # 计算相并面积\r\n",
    "    union = s1 + s2 - intersection\r\n",
    "    # 计算交并比\r\n",
    "    iou = intersection / union\r\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 12743 pred_boxes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "ANCHORS = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n",
    "ANCHOR_MASKS = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "VALID_THRESH = 0.01\n",
    "NMS_TOPK = 400\n",
    "NMS_POSK = 100\n",
    "NMS_THRESH = 0.45\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TRAINDIR = '/home/aistudio/work/insects/train/images'\n",
    "    TESTDIR = '/home/aistudio/work/insects/test/images'\n",
    "    VALIDDIR = '/home/aistudio/work/insects/val/images'\n",
    "    SAVEDIR = 'pred_result.txt'\n",
    "\n",
    "\n",
    "    model = YOLOv3(num_classes=NUM_CLASSES)\n",
    "    params_file_path = '/home/aistudio/yolo_epoch0'\n",
    "    model_state_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(model_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    total_results = []\n",
    "    test_loader = test_data_loader(VALIDDIR, batch_size= 1, mode='test')\n",
    "    for i, data in enumerate(test_loader()):\n",
    "        img_name, img_data, img_scale_data = data\n",
    "        img = paddle.to_tensor(img_data)\n",
    "        img_scale = paddle.to_tensor(img_scale_data)\n",
    "\n",
    "        outputs = model.forward(img)\n",
    "        bboxes, scores = model.get_pred(outputs,\n",
    "                                 im_shape=img_scale,\n",
    "                                 anchors=ANCHORS,\n",
    "                                 anchor_masks=ANCHOR_MASKS,\n",
    "                                 valid_thresh = VALID_THRESH)\n",
    "\n",
    "        bboxes_data = bboxes.numpy()\n",
    "        scores_data = scores.numpy()\n",
    "        result = multiclass_nms(bboxes_data, scores_data,\n",
    "                      score_thresh=VALID_THRESH, \n",
    "                      nms_thresh=NMS_THRESH, \n",
    "                      pre_nms_topk=NMS_TOPK, \n",
    "                      pos_nms_topk=NMS_POSK)\n",
    "        for j in range(len(result)):\n",
    "            result_j = np.array(result[j])\n",
    "            img_name_j = img_name[j]\n",
    "            for k in range(len(result_j)):\n",
    "                pred_k = result_j[k]\n",
    "                total_results.append(img_name_j + ' ' + str(pred_k[1]) + ' ' + str(pred_k[2]) + ' ' + str(pred_k[3]) + ' ' + str(pred_k[4]) + ' ' + str(pred_k[5]) + ' ' + str(pred_k[0]) + '\\n')\n",
    "    print('processed {} pred_boxes'.format(len(total_results)))\n",
    "\n",
    "    print('')\n",
    "    with open(SAVEDIR,\"w\") as f:\n",
    "        f.writelines(total_results)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 计算测试集上的mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def XXX_file_name(file_dir,Suffix_type):\r\n",
    "    L=[]\r\n",
    "    for root, dirs, files in os.walk(file_dir):\r\n",
    "        for file in files:\r\n",
    "            if os.path.splitext(file)[1] == \".\"+Suffix_type:\r\n",
    "                     L.append(os.path.join(root, file))\r\n",
    "    return L\r\n",
    "\r\n",
    "path='work/insects/val/images'\r\n",
    "file_list=XXX_file_name(path,'jpeg')\r\n",
    "name_list=[]\r\n",
    "for i in file_list:\r\n",
    "    i=i.split('/')[-1]\r\n",
    "    name_list.append(i.split('.jpeg')[0]+'\\n')\r\n",
    "with open('work/insects/val/val.txt',\"w\") as f:\r\n",
    "    f.writelines(name_list)\r\n",
    "f.close()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:128: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP for Boerner = 0.2345\n",
      "AP for Leconte = 0.6569\n",
      "AP for acuminatus = 0.0806\n",
      "AP for armandi = 0.1648\n",
      "AP for coleoptera = 0.0873\n",
      "AP for linnaeus = 0.0000\n",
      "Mean AP = 0.2040\n",
      "~~~~~~~~\n",
      "Results:\n",
      "0.235\n",
      "0.657\n",
      "0.081\n",
      "0.165\n",
      "0.087\n",
      "0.000\n",
      "0.204\n",
      "~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import os, sys, argparse\r\n",
    "import numpy as np\r\n",
    "import _pickle as cPickle\r\n",
    "import xml.etree.ElementTree as ET\r\n",
    "import os\r\n",
    "#import cPickle\r\n",
    "import _pickle as cPickle\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def parse_rec(filename):\r\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\r\n",
    "    tree = ET.parse(filename)\r\n",
    "    objects = []\r\n",
    "    for obj in tree.findall('object'):\r\n",
    "        obj_struct = {}\r\n",
    "        obj_struct['name'] = obj.find('name').text\r\n",
    "        #obj_struct['pose'] = obj.find('pose').text\r\n",
    "        #obj_struct['truncated'] = int(obj.find('truncated').text)\r\n",
    "        obj_struct['difficult'] = int(obj.find('difficult').text)\r\n",
    "        bbox = obj.find('bndbox')\r\n",
    "        obj_struct['bbox'] = [int(bbox.find('xmin').text),\r\n",
    "                              int(bbox.find('ymin').text),\r\n",
    "                              int(bbox.find('xmax').text),\r\n",
    "                              int(bbox.find('ymax').text)]\r\n",
    "        objects.append(obj_struct)\r\n",
    "\r\n",
    "    return objects\r\n",
    "\r\n",
    "def voc_ap(rec, prec, use_07_metric=False):\r\n",
    "    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\r\n",
    "    Compute VOC AP given precision and recall.\r\n",
    "    If use_07_metric is true, uses the\r\n",
    "    VOC 07 11 point method (default:False).\r\n",
    "    \"\"\"\r\n",
    "    if use_07_metric:\r\n",
    "        # 11 point metric\r\n",
    "        ap = 0.\r\n",
    "        for t in np.arange(0., 1.1, 0.1):\r\n",
    "            if np.sum(rec >= t) == 0:\r\n",
    "                p = 0\r\n",
    "            else:\r\n",
    "                p = np.max(prec[rec >= t])\r\n",
    "            ap = ap + p / 11.\r\n",
    "    else:\r\n",
    "        # correct AP calculation\r\n",
    "        # first append sentinel values at the end\r\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\r\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\r\n",
    "\r\n",
    "        # compute the precision envelope\r\n",
    "        for i in range(mpre.size - 1, 0, -1):\r\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\r\n",
    "\r\n",
    "        # to calculate area under PR curve, look for points\r\n",
    "        # where X axis (recall) changes value\r\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\r\n",
    "\r\n",
    "        # and sum (\\Delta recall) * prec\r\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\r\n",
    "    return ap\r\n",
    "\r\n",
    "def voc_eval(detpath,\r\n",
    "             annopath,\r\n",
    "             imagesetfile,\r\n",
    "             classname,\r\n",
    "             cachedir,\r\n",
    "             cls_to_num,\r\n",
    "             ovthresh=0.5,\r\n",
    "             use_07_metric=False,):\r\n",
    "\r\n",
    "    \"\"\"rec, prec, ap = voc_eval(detpath,\r\n",
    "                                annopath,\r\n",
    "                                imagesetfile,\r\n",
    "                                classname,\r\n",
    "                                [ovthresh],\r\n",
    "                                [use_07_metric])\r\n",
    "\r\n",
    "    Top level function that does the PASCAL VOC evaluation.\r\n",
    "\r\n",
    "    detpath: Path to detections\r\n",
    "        detpath.format(classname) should produce the detection results file.\r\n",
    "    annopath: Path to annotations\r\n",
    "            annopath.format(imagename) should be the xml annotations file.\r\n",
    "    imagesetfile: Text file containing the list of images, one image per line.\r\n",
    "    classname: Category name (duh)\r\n",
    "    cachedir: Directory for caching the annotations\r\n",
    "    [ovthresh]: Overlap threshold (default = 0.5)\r\n",
    "    [use_07_metric]: Whether to use VOC07's 11 point AP computation\r\n",
    "        (default False)\r\n",
    "    \"\"\"\r\n",
    "    # assumes detections are in detpath.format(classname)\r\n",
    "    # assumes annotations are in annopath.format(imagename)\r\n",
    "    # assumes imagesetfile is a text file with each line an image name\r\n",
    "    # cachedir caches the annotations in a pickle file\r\n",
    "\r\n",
    "    # first load gt\r\n",
    "    if not os.path.isdir(cachedir):\r\n",
    "        os.mkdir(cachedir)\r\n",
    "    cachefile = os.path.join(cachedir, 'annots.pkl')\r\n",
    "    # read list of images\r\n",
    "    with open(imagesetfile, 'r') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    imagenames = [x.strip() for x in lines]\r\n",
    "\r\n",
    "    if not os.path.isfile(cachefile):\r\n",
    "        # load annots\r\n",
    "        recs = {}\r\n",
    "        for i, imagename in enumerate(imagenames):\r\n",
    "            recs[imagename] = parse_rec(annopath.format(imagename))\r\n",
    "            #if i % 100 == 0:\r\n",
    "                #print('Reading annotation for {:d}/{:d}').format(i + 1, len(imagenames))\r\n",
    "        # save\r\n",
    "        #print('Saving cached annotations to {:s}').format(cachefile)\r\n",
    "        with open(cachefile, 'wb') as f:\r\n",
    "            cPickle.dump(recs, f)\r\n",
    "    else:\r\n",
    "        # load\r\n",
    "        # print('!!! cachefile = ',cachefile)\r\n",
    "        with open(cachefile, 'rb') as f:\r\n",
    "            recs = cPickle.load(f)\r\n",
    "\r\n",
    "    # extract gt objects for this class\r\n",
    "    class_recs = {}\r\n",
    "    npos = 0\r\n",
    "    for imagename in imagenames:\r\n",
    "        R = [obj for obj in recs[imagename] if obj['name'] == classname]\r\n",
    "        bbox = np.array([x['bbox'] for x in R])\r\n",
    "        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\r\n",
    "        det = [False] * len(R)\r\n",
    "        npos = npos + sum(~difficult)\r\n",
    "        class_recs[imagename] = {'bbox': bbox,\r\n",
    "                                 'difficult': difficult,\r\n",
    "                                 'det': det}\r\n",
    "\r\n",
    "    # read dets\r\n",
    "    detfile = detpath.format(classname)\r\n",
    "    cls_lines=[]\r\n",
    "\r\n",
    "    with open(detfile, 'r') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    for x in lines:\r\n",
    "        line=x.strip().split(' ')\r\n",
    "        if(float(line[-1])==float(cls_to_num[classname])):\r\n",
    "            cls_lines.append(x)\r\n",
    "\r\n",
    "\r\n",
    "    splitlines = [x.strip().split(' ') for x in cls_lines]\r\n",
    "    # print(splitlines)\r\n",
    "    image_ids = [x[0] for x in splitlines]\r\n",
    "    confidence = np.array([ float(x[1]) for x in splitlines])\r\n",
    "    BB = np.array([[float(z) for z in x[2:-1]] for x in splitlines])\r\n",
    "\r\n",
    "\r\n",
    "    sorted_ind = np.argsort(-confidence)\r\n",
    "    sorted_scores = np.sort(-confidence)\r\n",
    "    BB = BB[sorted_ind, :]\r\n",
    "    image_ids = [image_ids[x] for x in sorted_ind]\r\n",
    "\r\n",
    "    # go down dets and mark TPs and FPs\r\n",
    "    nd = len(image_ids)\r\n",
    "    tp = np.zeros(nd)\r\n",
    "    fp = np.zeros(nd)\r\n",
    "    for d in range(nd):\r\n",
    "        R = class_recs[image_ids[d]]\r\n",
    "        bb = BB[d, :].astype(float)\r\n",
    "        ovmax = -np.inf\r\n",
    "        BBGT = R['bbox'].astype(float)\r\n",
    "\r\n",
    "        if BBGT.size > 0:\r\n",
    "            # compute overlaps\r\n",
    "            # intersection\r\n",
    "            ixmin = np.maximum(BBGT[:, 0], bb[0])\r\n",
    "            iymin = np.maximum(BBGT[:, 1], bb[1])\r\n",
    "            ixmax = np.minimum(BBGT[:, 2], bb[2])\r\n",
    "            iymax = np.minimum(BBGT[:, 3], bb[3])\r\n",
    "            iw = np.maximum(ixmax - ixmin + 1., 0.)\r\n",
    "            ih = np.maximum(iymax - iymin + 1., 0.)\r\n",
    "            inters = iw * ih\r\n",
    "\r\n",
    "            # union\r\n",
    "            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\r\n",
    "                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\r\n",
    "                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\r\n",
    "\r\n",
    "            overlaps = inters / uni\r\n",
    "            ovmax = np.max(overlaps)\r\n",
    "            jmax = np.argmax(overlaps)\r\n",
    "\r\n",
    "        if ovmax > ovthresh:\r\n",
    "            if not R['difficult'][jmax]:\r\n",
    "                if not R['det'][jmax]:\r\n",
    "                    tp[d] = 1.\r\n",
    "                    R['det'][jmax] = 1\r\n",
    "                else:\r\n",
    "                    fp[d] = 1.\r\n",
    "        else:\r\n",
    "            fp[d] = 1.\r\n",
    "\r\n",
    "    # compute precision recall\r\n",
    "    fp = np.cumsum(fp)\r\n",
    "    tp = np.cumsum(tp)\r\n",
    "    rec = tp / float(npos)\r\n",
    "    # avoid divide by zero in case the first detection matches a difficult\r\n",
    "    # ground truth\r\n",
    "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\r\n",
    "    ap = voc_ap(rec, prec, use_07_metric)\r\n",
    "\r\n",
    "    return rec, prec, ap\r\n",
    "\r\n",
    "\r\n",
    "def do_python_eval(dir,annopath_dir,test_txt, classes, filename,cls_to_num):\r\n",
    "    cachedir = os.path.join(dir, 'annotations_cache')\r\n",
    "    aps = []\r\n",
    "\r\n",
    "    for i, cls in enumerate(classes):\r\n",
    "        if cls == '__background__':\r\n",
    "            continue\r\n",
    "        # print(cls)\r\n",
    "        rec, prec, ap = voc_eval(\r\n",
    "            filename, annopath_dir, test_txt, cls, cachedir,cls_to_num, ovthresh=0.5,\r\n",
    "            use_07_metric=1)\r\n",
    "        aps += [ap]\r\n",
    "        print('AP for {} = {:.4f}'.format(cls, ap))\r\n",
    "\r\n",
    "    print('Mean AP = {:.4f}'.format(np.mean(aps)))\r\n",
    "    print('~~~~~~~~')\r\n",
    "    print('Results:')\r\n",
    "    for ap in aps:\r\n",
    "        print('{:.3f}'.format(ap))\r\n",
    "    print('{:.3f}'.format(np.mean(aps)))\r\n",
    "    print('~~~~~~~~')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    classes = ['Boerner', 'Leconte', 'acuminatus','armandi', 'coleoptera','linnaeus']\r\n",
    "    cls_to_num={'Boerner': 0,\r\n",
    " 'Leconte': 1,\r\n",
    " 'Linnaeus': 2,\r\n",
    " 'acuminatus': 3,\r\n",
    " 'armandi': 4,\r\n",
    " 'coleoptera': 5,\r\n",
    " 'linnaeus': 6}\r\n",
    "    do_python_eval(\"\",\"work/insects/val/annotations/xmls/{}.xml\",\r\n",
    "                   \"work/insects/val/val.txt\",\r\n",
    "                   classes, \"pred_result.txt\",cls_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型效果及可视化展示\n",
    "<font size=3>\n",
    "  \n",
    "上面的程序展示了如何读取测试数据集的图片，并将最终结果保存在json格式的文件中。为了更直观的给读者展示模型效果，下面的程序添加了如何读取单张图片，并画出其产生的预测框。\n",
    "\n",
    "1. 创建数据读取器以读取单张图片的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取单张测试图片\n",
    "def single_image_data_loader(filename, test_image_size=608, mode='test'):\n",
    "    \"\"\"\n",
    "    加载测试用的图片，测试数据没有groundtruth标签\n",
    "    \"\"\"\n",
    "    batch_size= 1\n",
    "    def reader():\n",
    "        batch_data = []\n",
    "        img_size = test_image_size\n",
    "        file_path = os.path.join(filename)\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        H = img.shape[0]\n",
    "        W = img.shape[1]\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        mean = np.array(mean).reshape((1, 1, -1))\n",
    "        std = np.array(std).reshape((1, 1, -1))\n",
    "        out_img = (img / 255.0 - mean) / std\n",
    "        out_img = out_img.astype('float32').transpose((2, 0, 1))\n",
    "        img = out_img #np.transpose(out_img, (2,0,1))\n",
    "        im_shape = [H, W]\n",
    "\n",
    "        batch_data.append((image_name.split('.')[0], img, im_shape))\n",
    "        if len(batch_data) == batch_size:\n",
    "            yield make_test_array(batch_data)\n",
    "            batch_data = []\n",
    "\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "2. 定义绘制预测框的画图函数，代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义画图函数\n",
    "INSECT_NAMES = ['Boerner', 'Leconte', 'Linnaeus', \n",
    "                'acuminatus', 'armandi', 'coleoptera', 'linnaeus']\n",
    "\n",
    "# 定义画矩形框的函数 \n",
    "def draw_rectangle(currentAxis, bbox, edgecolor = 'k', facecolor = 'y', fill=False, linestyle='-'):\n",
    "    # currentAxis，坐标轴，通过plt.gca()获取\n",
    "    # bbox，边界框，包含四个数值的list， [x1, y1, x2, y2]\n",
    "    # edgecolor，边框线条颜色\n",
    "    # facecolor，填充颜色\n",
    "    # fill, 是否填充\n",
    "    # linestype，边框线型\n",
    "    # patches.Rectangle需要传入左上角坐标、矩形区域的宽度、高度等参数\n",
    "    rect=patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1, linewidth=1,\n",
    "                           edgecolor=edgecolor,facecolor=facecolor,fill=fill, linestyle=linestyle)\n",
    "    currentAxis.add_patch(rect)\n",
    "\n",
    "# 定义绘制预测结果的函数\n",
    "def draw_results(result, filename, draw_thresh=0.5):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    im = imread(filename)\n",
    "    plt.imshow(im)\n",
    "    currentAxis=plt.gca()\n",
    "    colors = ['r', 'g', 'b', 'k', 'y', 'c', 'purple']\n",
    "    for item in result:\n",
    "        box = item[2:6]\n",
    "        label = int(item[0])\n",
    "        name = INSECT_NAMES[label]\n",
    "        if item[1] > draw_thresh:\n",
    "            draw_rectangle(currentAxis, box, edgecolor = colors[label])\n",
    "            plt.text(box[0], box[1], name, fontsize=12, color=colors[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "3. 使用上面定义的single_image_data_loader函数读取指定的图片，输入网络并计算出预测框和得分，然后使用多分类非极大值抑制消除冗余的框。将最终结果画图展示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import paddle\n",
    "\n",
    "ANCHORS = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n",
    "ANCHOR_MASKS = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "VALID_THRESH = 0.01\n",
    "NMS_TOPK = 400\n",
    "NMS_POSK = 100\n",
    "NMS_THRESH = 0.45\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "if __name__ == '__main__':\n",
    "    image_name = '/home/aistudio/work/insects/test/images/2599.jpeg'\n",
    "    params_file_path = '/home/aistudio/yolo_epoch0'\n",
    "\n",
    "    model = YOLOv3(num_classes=NUM_CLASSES)\n",
    "    model_state_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(model_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    total_results = []\n",
    "    test_loader = single_image_data_loader(image_name, mode='test')\n",
    "    for i, data in enumerate(test_loader()):\n",
    "        img_name, img_data, img_scale_data = data\n",
    "        img = paddle.to_tensor(img_data)\n",
    "        img_scale = paddle.to_tensor(img_scale_data)\n",
    "\n",
    "        outputs = model.forward(img)\n",
    "        bboxes, scores = model.get_pred(outputs,\n",
    "                                 im_shape=img_scale,\n",
    "                                 anchors=ANCHORS,\n",
    "                                 anchor_masks=ANCHOR_MASKS,\n",
    "                                 valid_thresh = VALID_THRESH)\n",
    "\n",
    "        bboxes_data = bboxes.numpy()\n",
    "        scores_data = scores.numpy()\n",
    "        results = multiclass_nms(bboxes_data, scores_data,\n",
    "                      score_thresh=VALID_THRESH, \n",
    "                      nms_thresh=NMS_THRESH, \n",
    "                      pre_nms_topk=NMS_TOPK, \n",
    "                      pos_nms_topk=NMS_POSK)\n",
    "\n",
    "result = results[0]\n",
    "draw_results(result, image_name, draw_thresh=0.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=3>\n",
    "  \n",
    "  通过上面的程序，清晰的给读者展示了如何使用训练好的权重，对图片进行预测并将结果可视化。最终输出的图片上，检测出了每个昆虫，标出了它们的边界框和具体类别。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
