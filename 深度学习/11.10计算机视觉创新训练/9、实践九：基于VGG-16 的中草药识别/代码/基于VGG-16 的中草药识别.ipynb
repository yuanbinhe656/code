{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **任务描述：**\n",
    "\n",
    "\n",
    "如何根据据图像的视觉内容为图像赋予一个语义类别（例如，教室、街道等）是图像场景分类的目标，也是图像检索、图像内容分析和目标识别等问题的基础。但由于图片的尺度、角度、光照等多样性以及场景定义的复杂性，场景分类一直是计算机视觉中的一个挑战性问题。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**本实践旨在通过一个场景分类的案列，让大家理解和掌握如何使用飞桨动态图搭建一个经典的卷积神经网络。**\n",
    "\n",
    "<br/>\n",
    "\n",
    "**特别提示：本实践所用数据集均来自互联网，请勿用于商务用途。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "2021-08-23 21:59:19,631 - INFO - font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2021-08-23 21:59:19,964 - INFO - generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import paddle\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.io import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "参数配置\n",
    "'''\n",
    "train_parameters = {\n",
    "    \"input_size\": [3, 224, 224],                              #输入图片的shape\n",
    "    \"class_dim\": -1,                                          #分类数\n",
    "    \"src_path\":\"/home/aistudio/data/data55190/Chinese Medicine.zip\",    #原始数据集路径\n",
    "    \"target_path\":\"/home/aistudio/data/\",                     #要解压的路径\n",
    "    \"train_list_path\": \"/home/aistudio/data/train.txt\",       #train.txt路径\n",
    "    \"eval_list_path\": \"/home/aistudio/data/eval.txt\",         #eval.txt路径\n",
    "    \"readme_path\": \"/home/aistudio/data/readme.json\",         #readme.json路径\n",
    "    \"label_dict\":{},                                          #标签字典\n",
    "    \"num_epochs\": 1,                                         #训练轮数\n",
    "    \"train_batch_size\": 8,                                    #训练时每个批次的大小\n",
    "    \"skip_steps\": 10,\n",
    "    \"save_steps\": 30, \n",
    "    \"learning_strategy\": {                                    #优化函数相关的配置\n",
    "        \"lr\": 0.0001                                          #超参数学习率\n",
    "    },\n",
    "    \"checkpoints\": \"/home/aistudio/work/checkpoints\"          #保存的路径\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **一、数据准备**\n",
    "\n",
    "（1）解压原始数据集\n",
    "\n",
    "（2）按照比例划分训练集与验证集\n",
    "\n",
    "（3）乱序，生成数据列表\n",
    "\n",
    "（4）定义数据读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unzip_data(src_path,target_path):\n",
    "    '''\n",
    "    解压原始数据集，将src_path路径下的zip包解压至target_path目录下\n",
    "    '''\n",
    "    if(not os.path.isdir(target_path + \"Chinese Medicine\")):     \n",
    "        z = zipfile.ZipFile(src_path, 'r')\n",
    "        z.extractall(path=target_path)\n",
    "        z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_list(target_path,train_list_path,eval_list_path):\n",
    "    '''\n",
    "    生成数据列表\n",
    "    '''\n",
    "    #存放所有类别的信息\n",
    "    class_detail = []\n",
    "    #获取所有类别保存的文件夹名称\n",
    "    data_list_path=target_path+\"Chinese Medicine/\"\n",
    "    class_dirs = os.listdir(data_list_path)  \n",
    "    #总的图像数量\n",
    "    all_class_images = 0\n",
    "    #存放类别标签\n",
    "    class_label=0\n",
    "    #存放类别数目\n",
    "    class_dim = 0\n",
    "    #存储要写进eval.txt和train.txt中的内容\n",
    "    trainer_list=[]\n",
    "    eval_list=[]\n",
    "    #读取每个类别，['river', 'lawn','church','ice','desert']\n",
    "    for class_dir in class_dirs:\n",
    "        if class_dir != \".DS_Store\":\n",
    "            class_dim += 1\n",
    "            #每个类别的信息\n",
    "            class_detail_list = {}\n",
    "            eval_sum = 0\n",
    "            trainer_sum = 0\n",
    "            #统计每个类别有多少张图片\n",
    "            class_sum = 0\n",
    "            #获取类别路径 \n",
    "            path = data_list_path  + class_dir\n",
    "            # 获取所有图片\n",
    "            img_paths = os.listdir(path)\n",
    "            for img_path in img_paths:                                  # 遍历文件夹下的每个图片\n",
    "                name_path = path + '/' + img_path                       # 每张图片的路径\n",
    "                if class_sum % 8 == 0:                                  # 每8张图片取一个做验证数据\n",
    "                    eval_sum += 1                                       # test_sum为测试数据的数目\n",
    "                    eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")\n",
    "                else:\n",
    "                    trainer_sum += 1 \n",
    "                    trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")#trainer_sum测试数据的数目\n",
    "                class_sum += 1                                          #每类图片的数目\n",
    "                all_class_images += 1                                   #所有类图片的数目\n",
    "             \n",
    "            # 说明的json文件的class_detail数据\n",
    "            class_detail_list['class_name'] = class_dir             #类别名称\n",
    "            class_detail_list['class_label'] = class_label          #类别标签\n",
    "            class_detail_list['class_eval_images'] = eval_sum       #该类数据的测试集数目\n",
    "            class_detail_list['class_trainer_images'] = trainer_sum #该类数据的训练集数目\n",
    "            class_detail.append(class_detail_list)  \n",
    "            #初始化标签列表\n",
    "            train_parameters['label_dict'][str(class_label)] = class_dir\n",
    "            class_label += 1 \n",
    "            \n",
    "    #初始化分类数\n",
    "    train_parameters['class_dim'] = class_dim\n",
    "  \n",
    "    #乱序  \n",
    "    random.shuffle(eval_list)\n",
    "    with open(eval_list_path, 'a') as f:\n",
    "        for eval_image in eval_list:\n",
    "            f.write(eval_image) \n",
    "            \n",
    "    random.shuffle(trainer_list)\n",
    "    with open(train_list_path, 'a') as f2:\n",
    "        for train_image in trainer_list:\n",
    "            f2.write(train_image) \n",
    "\n",
    "    # 说明的json文件信息\n",
    "    readjson = {}\n",
    "    readjson['all_class_name'] = data_list_path                  #文件父目录\n",
    "    readjson['all_class_images'] = all_class_images\n",
    "    readjson['class_detail'] = class_detail\n",
    "    jsons = json.dumps(readjson, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    with open(train_parameters['readme_path'],'w') as f:\n",
    "        f.write(jsons)\n",
    "    print ('生成数据列表完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成数据列表完成！\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "参数初始化\n",
    "'''\n",
    "src_path=train_parameters['src_path']\n",
    "target_path=train_parameters['target_path']\n",
    "train_list_path=train_parameters['train_list_path']\n",
    "eval_list_path=train_parameters['eval_list_path']\n",
    "\n",
    "'''\n",
    "解压原始数据到指定路径\n",
    "'''\n",
    "unzip_data(src_path,target_path)\n",
    "\n",
    "'''\n",
    "划分训练集与验证集，乱序，生成数据列表\n",
    "'''\n",
    "#每次生成数据列表前，首先清空train.txt和eval.txt\n",
    "with open(train_list_path, 'w') as f: \n",
    "    f.seek(0)\n",
    "    f.truncate() \n",
    "with open(eval_list_path, 'w') as f: \n",
    "    f.seek(0)\n",
    "    f.truncate() \n",
    "    \n",
    "#生成数据列表   \n",
    "get_data_list(target_path,train_list_path,eval_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\r\n",
    "    def __init__(self, data_path, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        数据读取器\r\n",
    "        :param data_path: 数据集所在路径\r\n",
    "        :param mode: train or eval\r\n",
    "        \"\"\"\r\n",
    "        super().__init__()\r\n",
    "        self.data_path = data_path\r\n",
    "        self.img_paths = []\r\n",
    "        self.labels = []\r\n",
    "\r\n",
    "        if mode == 'train':\r\n",
    "            with open(os.path.join(self.data_path, \"train.txt\"), \"r\", encoding=\"utf-8\") as f:\r\n",
    "                self.info = f.readlines()\r\n",
    "            for img_info in self.info:\r\n",
    "                img_path, label = img_info.strip().split('\\t')\r\n",
    "                self.img_paths.append(img_path)\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "        else:\r\n",
    "            with open(os.path.join(self.data_path, \"eval.txt\"), \"r\", encoding=\"utf-8\") as f:\r\n",
    "                self.info = f.readlines()\r\n",
    "            for img_info in self.info:\r\n",
    "                img_path, label = img_info.strip().split('\\t')\r\n",
    "                self.img_paths.append(img_path)\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        获取一组数据\r\n",
    "        :param index: 文件索引号\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        # 第一步打开图像文件并获取label值\r\n",
    "        img_path = self.img_paths[index]\r\n",
    "        img = Image.open(img_path)\r\n",
    "        if img.mode != 'RGB':\r\n",
    "            img = img.convert('RGB') \r\n",
    "        img = img.resize((224, 224), Image.BILINEAR)\r\n",
    "        img = np.array(img).astype('float32')\r\n",
    "        img = img.transpose((2, 0, 1)) / 255\r\n",
    "        label = self.labels[index]\r\n",
    "        label = np.array([label], dtype=\"int64\")\r\n",
    "        return img, label\r\n",
    "\r\n",
    "    def print_sample(self, index: int = 0):\r\n",
    "        print(\"文件名\", self.img_paths[index], \"\\t标签值\", self.labels[index])\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "#训练数据加载\r\n",
    "train_dataset = dataset('/home/aistudio/data',mode='train')\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=16, shuffle=True)\r\n",
    "#测试数据加载\r\n",
    "eval_dataset = dataset('/home/aistudio/data',mode='eval')\r\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size = 8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件名 /home/aistudio/data/Chinese Medicine/dangshen/dangshen_207.jpg \t标签值 0\n",
      "787\n",
      "文件名 /home/aistudio/data/Chinese Medicine/dangshen/dangshen_124.jpg \t标签值 0\n",
      "115\n",
      "(3, 224, 224)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset.print_sample(200)\n",
    "print(train_dataset.__len__())\n",
    "eval_dataset.print_sample(0)\n",
    "print(eval_dataset.__len__())\n",
    "print(eval_dataset.__getitem__(10)[0].shape)\n",
    "print(eval_dataset.__getitem__(10)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **二、模型配置**\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9ca0744272b0449186f766afcabadd598e24679088a4438dafede05a71b7c141)\n",
    "\n",
    "VGG的核心是五组卷积操作，每两组之间做Max-Pooling空间降维。同一组内采用多次连续的3X3卷积，卷积核的数目由较浅组的64增多到最深组的512，同一组内的卷积核数目是一样的。卷积之后接两层全连 接层，之后是分类层。由于每组内卷积层的不同，有11、13、16、19层这几种模型，上图展示一个16层的网络结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvPool(paddle.nn.Layer):\n",
    "    '''卷积+池化'''\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 num_filters, \n",
    "                 filter_size,\n",
    "                 pool_size,\n",
    "                 pool_stride,\n",
    "                 groups,\n",
    "                 conv_stride=1, \n",
    "                 conv_padding=1,\n",
    "                 ):\n",
    "        super(ConvPool, self).__init__()  \n",
    "\n",
    "\n",
    "        for i in range(groups):\n",
    "            self.add_sublayer(   #添加子层实例\n",
    "                'bb_%d' % i,\n",
    "                paddle.nn.Conv2D(         # layer\n",
    "                in_channels=num_channels, #通道数\n",
    "                out_channels=num_filters,   #卷积核个数\n",
    "                kernel_size=filter_size,   #卷积核大小\n",
    "                stride=conv_stride,        #步长\n",
    "                padding = conv_padding,    #padding\n",
    "                )\n",
    "            )\n",
    "            self.add_sublayer(\n",
    "                'relu%d' % i,\n",
    "                paddle.nn.ReLU()\n",
    "            )\n",
    "            num_channels = num_filters\n",
    "            \n",
    "\n",
    "        self.add_sublayer(\n",
    "            'Maxpool',\n",
    "            paddle.nn.MaxPool2D(\n",
    "            kernel_size=pool_size,           #池化核大小\n",
    "            stride=pool_stride               #池化步长\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for prefix, sub_layer in self.named_children():\n",
    "            # print(prefix,sub_layer)\n",
    "            x = sub_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VGGNet(paddle.nn.Layer):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()       \n",
    "        self.convpool01 = ConvPool(\n",
    "            3, 64, 3, 2, 2, 2)  #3:通道数，64：卷积核个数，3:卷积核大小，2:池化核大小，2:池化步长，2:连续卷积个数\n",
    "        self.convpool02 = ConvPool(\n",
    "            64, 128, 3, 2, 2, 2)\n",
    "        self.convpool03 = ConvPool(\n",
    "            128, 256, 3, 2, 2, 3) \n",
    "        self.convpool04 = ConvPool(\n",
    "            256, 512, 3, 2, 2, 3)\n",
    "        self.convpool05 = ConvPool(\n",
    "            512, 512, 3, 2, 2, 3)       \n",
    "        self.pool_5_shape = 512 * 7* 7\n",
    "        self.fc01 = paddle.nn.Linear(self.pool_5_shape, 4096)\n",
    "        self.fc02 = paddle.nn.Linear(4096, 4096)\n",
    "        self.fc03 = paddle.nn.Linear(4096, train_parameters['class_dim'])\n",
    "\n",
    "    def forward(self, inputs, label=None):\n",
    "        # print('input_shape:', inputs.shape) #[8, 3, 224, 224]\n",
    "        \"\"\"前向计算\"\"\"\n",
    "        out = self.convpool01(inputs) \n",
    "        out = self.convpool02(out) \n",
    "        out = self.convpool03(out) \n",
    "        out = self.convpool04(out) \n",
    "        out = self.convpool05(out)      \n",
    "\n",
    "        out = paddle.reshape(out, shape=[-1, 512*7*7])\n",
    "        out = self.fc01(out)\n",
    "        out = self.fc02(out)\n",
    "        out = self.fc03(out)\n",
    "        \n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=out, label=label)\n",
    "            return out, acc\n",
    "        else:\n",
    "            return out\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、模型训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_process(title,color,iters,data,label):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'0': 'dangshen', '1': 'jinyinhua', '2': 'gouqi', '3': 'huaihua', '4': 'baihe'}\n"
     ]
    }
   ],
   "source": [
    "print(train_parameters['class_dim'])\n",
    "print(train_parameters['label_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epo: 0, step: 10, loss is: [1.6257975], acc is: [0.25]\n",
      "epo: 0, step: 20, loss is: [1.5391165], acc is: [0.125]\n",
      "epo: 0, step: 30, loss is: [1.1957078], acc is: [0.375]\n",
      "save model to: /home/aistudio/work/checkpoints/save_dir_30.pdparams\n",
      "epo: 0, step: 40, loss is: [1.1457045], acc is: [0.625]\n",
      "epo: 0, step: 50, loss is: [1.0343454], acc is: [0.33333334]\n"
     ]
    }
   ],
   "source": [
    "model = VGGNet()\n",
    "model.train()\n",
    "cross_entropy = paddle.nn.CrossEntropyLoss()\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=train_parameters['learning_strategy']['lr'],\n",
    "                                  parameters=model.parameters()) \n",
    "\n",
    "steps = 0\n",
    "Iters, total_loss, total_acc = [], [], []\n",
    "\n",
    "for epo in range(train_parameters['num_epochs']):\n",
    "    for _, data in enumerate(train_loader()):\n",
    "        steps += 1\n",
    "        x_data = data[0]\n",
    "        y_data = data[1]\n",
    "        predicts, acc = model(x_data, y_data)\n",
    "        loss = cross_entropy(predicts, y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        if steps % train_parameters[\"skip_steps\"] == 0:\n",
    "            Iters.append(steps)\n",
    "            total_loss.append(loss.numpy()[0])\n",
    "            total_acc.append(acc.numpy()[0])\n",
    "            #打印中间过程\n",
    "            print('epo: {}, step: {}, loss is: {}, acc is: {}'\\\n",
    "                  .format(epo, steps, loss.numpy(), acc.numpy()))\n",
    "        #保存模型参数\n",
    "        if steps % train_parameters[\"save_steps\"] == 0:\n",
    "            save_path = train_parameters[\"checkpoints\"]+\"/\"+\"save_dir_\" + str(steps) + '.pdparams'\n",
    "            print('save model to: ' + save_path)\n",
    "            paddle.save(model.state_dict(),save_path)\n",
    "paddle.save(model.state_dict(),train_parameters[\"checkpoints\"]+\"/\"+\"save_dir_final.pdparams\")\n",
    "draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "模型评估\n",
    "'''\n",
    "model__state_dict = paddle.load('work/checkpoints/save_dir_final.pdparams')\n",
    "model_eval = VGGNet()\n",
    "model_eval.set_state_dict(model__state_dict) \n",
    "model_eval.eval()\n",
    "accs = []\n",
    "\n",
    "for _, data in enumerate(eval_loader()):\n",
    "    x_data = data[0]\n",
    "    y_data = data[1]\n",
    "    predicts = model_eval(x_data)\n",
    "    acc = paddle.metric.accuracy(predicts, y_data)\n",
    "    accs.append(acc.numpy()[0])\n",
    "print('模型在验证集上的准确率为：',np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **五、模型预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unzip_infer_data(src_path,target_path):\n",
    "    '''\n",
    "    解压预测数据集\n",
    "    '''\n",
    "    if(not os.path.isdir(target_path + \"Chinese Medicine Infer\")):     \n",
    "        z = zipfile.ZipFile(src_path, 'r')\n",
    "        z.extractall(path=target_path)\n",
    "        z.close()\n",
    "\n",
    "\n",
    "def load_image(img_path):\n",
    "    '''\n",
    "    预测图片预处理\n",
    "    '''\n",
    "    img = Image.open(img_path) \n",
    "    if img.mode != 'RGB': \n",
    "        img = img.convert('RGB') \n",
    "    img = img.resize((224, 224), Image.BILINEAR)\n",
    "    img = np.array(img).astype('float32') \n",
    "    img = img.transpose((2, 0, 1)) / 255 # HWC to CHW 及归一化\n",
    "    return img\n",
    "\n",
    "\n",
    "infer_src_path = '/home/aistudio/data/data55194/Chinese Medicine Infer.zip'\n",
    "infer_dst_path = '/home/aistudio/data/'\n",
    "unzip_infer_data(infer_src_path,infer_dst_path)\n",
    "\n",
    "label_dic = train_parameters['label_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model__state_dict = paddle.load('work/checkpoints/save_dir_final.pdparams')\n",
    "model_predict = VGGNet()\n",
    "model_predict.set_state_dict(model__state_dict) \n",
    "model_predict.eval()\n",
    "infer_imgs_path = os.listdir(infer_dst_path+\"Chinese Medicine Infer\")\n",
    "print(infer_imgs_path)\n",
    "for infer_img_path in infer_imgs_path:\n",
    "    infer_img = load_image(infer_dst_path+\"Chinese Medicine Infer/\"+infer_img_path)\n",
    "    infer_img = infer_img[np.newaxis,:, : ,:]  #reshape(-1,3,224,224)\n",
    "    infer_img = paddle.to_tensor(infer_img)\n",
    "    result = model_predict(infer_img)\n",
    "    lab = np.argmax(result.numpy())\n",
    "    print(\"样本: {},被预测为:{}\".format(infer_img_path,label_dic[str(lab)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
